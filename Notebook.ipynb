{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, tv_tensors\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from typing import Union,TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/train/_annotations.coco.json') as file:\n",
    "    my_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n",
      "[{'id': 0, 'image_id': 0, 'category_id': 3, 'bbox': [259, 49, 4.8, 9.6], 'area': 46.08, 'segmentation': [[264, 48.8, 259.2, 48.8, 259.2, 58.4, 264, 58.4, 264, 48.8]], 'iscrowd': 0}, {'id': 1, 'image_id': 0, 'category_id': 3, 'bbox': [284, 630, 4.8, 8.8], 'area': 42.24, 'segmentation': [[288.8, 630.4, 284, 630.4, 284, 639.2, 288.8, 639.2, 288.8, 630.4]], 'iscrowd': 0}]\n",
      "[{'id': 0, 'license': 1, 'file_name': 'P2491__1-0__1200___1764_png_jpg.rf.00342c6c14ae53b3bfadd7995643e1bc.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-12-20T13:55:24+00:00'}, {'id': 1, 'license': 1, 'file_name': '4f833867-273e-4d73-8bc3-cb2d9ceb54ef_0_0_jpg.rf.000c42e196c096916dfe7c0744d06e12.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-12-20T13:55:24+00:00'}]\n",
      "[{'id': 1, 'name': 'Aircraft'}, {'id': 2, 'name': 'ship'}, {'id': 3, 'name': 'vehicle'}]\n"
     ]
    }
   ],
   "source": [
    "print(my_dict.keys())\n",
    "print(my_dict['annotations'][0:2])\n",
    "print(my_dict['images'][0:2])\n",
    "print(my_dict['categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AerialViewDataset(Dataset):\n",
    "    def __init__(self, root_folder) -> None:\n",
    "        super().__init__()\n",
    "        self.root_folder = root_folder\n",
    "\n",
    "        with open(f'{self.root_folder}/_annotations.coco.json') as file:\n",
    "            self.raw_dictionary = json.load(file)\n",
    "\n",
    "        self.list_of_image_dictionaries = self.raw_dictionary['images']\n",
    "\n",
    "        # Adds all the annotations to the dictionary containing a particular images' details under the key 'list_of_annotations'\n",
    "        for dict in self.list_of_image_dictionaries:\n",
    "            dict['list_of_annotations'] = []\n",
    "            for annotation in self.raw_dictionary['annotations']:\n",
    "                if annotation['image_id'] == dict['id']:\n",
    "                    dict['list_of_annotations'].append(annotation)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple containing 'Image' and 'Target'\n",
    "        \"\"\"\n",
    "        image_path = f'{self.root_folder}' + self.raw_dictionary['images'][index]['file_name']\n",
    "        transformer = transforms.PILToTensor()\n",
    "        with Image.open(image_path) as pil_image:\n",
    "            image = transformer(pil_image)\n",
    "            image = image.float()\n",
    "        target = {}\n",
    "        target['area'] = []\n",
    "        target['boxes'] = []\n",
    "        target['image_id'] = torch.tensor(self.list_of_image_dictionaries[index]['id'])\n",
    "        target['labels'] = []\n",
    "\n",
    "        for annotation in self.list_of_image_dictionaries[index]['list_of_annotations']:\n",
    "            target['area'].append(annotation['area'])\n",
    "            target['boxes'].append(annotation['bbox'])\n",
    "            target['labels'].append(annotation['category_id'])\n",
    "        \n",
    "        target['area'] = torch.Tensor(target['area']).float()\n",
    "\n",
    "        # Convert the boxes attribute to tensors and then format it to xyxy from xywh\n",
    "        target['boxes'] = tv_tensors.BoundingBoxes(target['boxes'], format='xywh', canvas_size=(640, 640))\n",
    "        target['boxes'] = torchvision.ops.box_convert(target['boxes'],  in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "        target['labels'] = torch.Tensor(target['labels']).long()\n",
    "\n",
    "        return (image, target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_dictionary['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AerialViewDataset('Datasets/train/')\n",
    "valid_dataset = AerialViewDataset('Datasets/valid/')\n",
    "test_dataset = AerialViewDataset('Datasets/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': tensor([44.8000, 33.2800, 35.8400, 40.9600, 38.4000]),\n",
       " 'boxes': tensor([[293.0000, 293.0000, 297.0000, 304.2000],\n",
       "         [299.0000, 109.0000, 302.2000, 119.4000],\n",
       "         [298.0000,  60.0000, 301.2000,  71.2000],\n",
       "         [322.0000, 147.0000, 325.2000, 159.8000],\n",
       "         [318.0000, 154.0000, 321.2000, 166.0000]]),\n",
       " 'image_id': tensor(1),\n",
       " 'labels': tensor([3, 3, 3, 3, 3])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bounding_boxes(image:Union[Image.Image, torch.tensor], bounding_boxes:tv_tensors.BoundingBoxes):\n",
    "    if type(image) == Image.Image:\n",
    "        transformer = transforms.PILToTensor()\n",
    "        image = transformer(image)\n",
    "    image = image.byte()\n",
    "    annotated_image_tensor = torchvision.utils.draw_bounding_boxes(image, bounding_boxes, colors='green')\n",
    "    transforms.functional.to_pil_image(annotated_image_tensor).show()\n",
    "    \n",
    "def show_bounding_boxes_from_image_id(image_id:int, root_path:str = 'Datasets/train/', list_of_images:list = my_dict['images'], list_of_annotations:list = my_dict['annotations']):\n",
    "    raw_box_coords = []\n",
    "    for annotation in list_of_annotations:\n",
    "        if annotation['image_id'] == image_id:\n",
    "            raw_box_coords.append(annotation['bbox'])\n",
    "    \n",
    "    tensored_box_coords = tv_tensors.BoundingBoxes(raw_box_coords, format='xywh', canvas_size=(640, 640))\n",
    "    formatted_box_coords = torchvision.ops.box_convert(tensored_box_coords, in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "    image_path = f'{root_path}' + list_of_images[image_id]['file_name']\n",
    "\n",
    "    with Image.open(image_path) as image:\n",
    "        show_bounding_boxes(image, formatted_box_coords)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bounding_boxes(test_dataset[161][0], test_dataset[161][1]['boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32090/833195210.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_parameters = torch.load('weights/best_model.pt', map_location=torch.device(device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dir(model)\n",
    "#model.modules\n",
    "num_classes = 4 \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_parameters = torch.load('weights/best_model.pt', map_location=torch.device(device))\n",
    "model.load_state_dict(model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop. Takes in a model, the training and validation data loaders, the number of epochs and the initial learning rate\n",
    "def train(model, train_loader, validation_loader, epochs = 10, learning_rate = 1, model_name:str = \"My Model\"):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set the optimiser to be an instance of the stochastic gradient descent class\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimiser = torch.optim.SGD(parameters, lr=learning_rate)\n",
    "\n",
    "    # Define a learning rate scheduler as an instance of the ReduceLROnPlateau class\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=50, cooldown=7, eps=1e-20)\n",
    "\n",
    "    # Writer will be used to track model performance with TensorBoard\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Keep track of the number of batches to plot model performace against\n",
    "    batch_index = 0\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Within each epoch, we pass through the entire training data in batches indexed by batch\n",
    "        for batch in train_loader:\n",
    "            # Loads features and labels into device for performance improvements\n",
    "            features, labels = batch\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            features = list(img.to(device) for img in features)\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            loss_dict = model(features, labels)\n",
    "\n",
    "            # Calculate the loss via cross_entropy\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Create the grad attributes\n",
    "            loss.backward() \n",
    "\n",
    "            # Clip the loss value so it doesn't become NaN\n",
    "            torch.nn.utils.clip_grad_norm_(parameters, 4)\n",
    "\n",
    "            # Print the performance\n",
    "            print(f\"Epoch: {epoch}, batch index: {batch_index}, learning rate: {scheduler.get_last_lr()}, loss:{loss.item()}\")\n",
    "\n",
    "            # Perform one step of stochastic gradient descent\n",
    "            optimiser.step()\n",
    "\n",
    "            # Zero the gradients (Apparently set_to_none=True imporves performace)\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Feed the loss amount into the learning rate scheduler to decide the next learning rate\n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "            # Write the performance to the TensorBoard plot\n",
    "            writer.add_scalar('loss', loss.item(), batch_index)\n",
    "\n",
    "            # Increment the batch index\n",
    "            batch_index += 1\n",
    "\n",
    "            print(torch.cuda.memory_allocated())\n",
    "            torch.cuda.memory_summary()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "def accuracy_score_from_valiadation(model, validation_loader):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy using the WHOLE of the validation dataset.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        losses = torch.zeros(0).to(device)\n",
    "\n",
    "        for batch_index, batch in enumerate(validation_loader):\n",
    "\n",
    "            features, labels = batch\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            features = list(img.to(device) for img in features)\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            loss_dict = model(features, labels)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            losses = torch.cat((losses, loss.view(1)))\n",
    "\n",
    "        accuracy_score = torch.sum(losses) / len(losses)\n",
    "\n",
    "        return accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy score1.0177878141403198\n",
      "Epoch: 0, batch index: 0, learning rate: [0.01], loss:0.7987294793128967\n",
      "375135232\n",
      "Epoch: 0, batch index: 1, learning rate: [0.01], loss:1.4860544204711914\n",
      "375136768\n",
      "Epoch: 0, batch index: 2, learning rate: [0.01], loss:0.9288530349731445\n",
      "375136768\n",
      "Epoch: 0, batch index: 3, learning rate: [0.01], loss:1.024763584136963\n",
      "375135232\n",
      "Epoch: 0, batch index: 4, learning rate: [0.01], loss:1.821382999420166\n",
      "375139328\n",
      "Epoch: 0, batch index: 5, learning rate: [0.01], loss:1.2870628833770752\n",
      "375135232\n",
      "Epoch: 0, batch index: 6, learning rate: [0.01], loss:1.7455857992172241\n",
      "375153152\n",
      "Epoch: 0, batch index: 7, learning rate: [0.01], loss:1.1948108673095703\n",
      "375135232\n",
      "Epoch: 0, batch index: 8, learning rate: [0.01], loss:1.029102087020874\n",
      "375137280\n",
      "Epoch: 0, batch index: 9, learning rate: [0.01], loss:1.408943772315979\n",
      "375135744\n",
      "Epoch: 0, batch index: 10, learning rate: [0.01], loss:0.898222804069519\n",
      "375135744\n",
      "Epoch: 0, batch index: 11, learning rate: [0.01], loss:1.965562105178833\n",
      "375135744\n",
      "Epoch: 0, batch index: 12, learning rate: [0.01], loss:1.3394908905029297\n",
      "375136768\n",
      "Epoch: 0, batch index: 13, learning rate: [0.01], loss:1.6695992946624756\n",
      "375135744\n",
      "Epoch: 0, batch index: 14, learning rate: [0.01], loss:0.8855596780776978\n",
      "375136768\n",
      "Epoch: 0, batch index: 15, learning rate: [0.01], loss:0.7729510068893433\n",
      "375135232\n",
      "Epoch: 0, batch index: 16, learning rate: [0.01], loss:0.593059778213501\n",
      "375135232\n",
      "Epoch: 0, batch index: 17, learning rate: [0.01], loss:0.9666456580162048\n",
      "375165440\n",
      "Epoch: 0, batch index: 18, learning rate: [0.01], loss:1.5135596990585327\n",
      "375135232\n",
      "Epoch: 0, batch index: 19, learning rate: [0.01], loss:0.9525017738342285\n",
      "375135744\n",
      "Epoch: 0, batch index: 20, learning rate: [0.01], loss:0.9319621324539185\n",
      "375135232\n",
      "Epoch: 0, batch index: 21, learning rate: [0.01], loss:0.4282692074775696\n",
      "375135232\n",
      "Epoch: 0, batch index: 22, learning rate: [0.01], loss:1.7278029918670654\n",
      "375138816\n",
      "Epoch: 0, batch index: 23, learning rate: [0.01], loss:0.9036171436309814\n",
      "375136256\n",
      "Epoch: 0, batch index: 24, learning rate: [0.01], loss:1.2160261869430542\n",
      "375135232\n",
      "Epoch: 0, batch index: 25, learning rate: [0.01], loss:1.3697175979614258\n",
      "375146496\n",
      "Epoch: 0, batch index: 26, learning rate: [0.01], loss:1.668015480041504\n",
      "375135744\n",
      "Epoch: 0, batch index: 27, learning rate: [0.01], loss:1.6631779670715332\n",
      "375136768\n",
      "Epoch: 0, batch index: 28, learning rate: [0.01], loss:1.1074037551879883\n",
      "375135232\n",
      "Epoch: 0, batch index: 29, learning rate: [0.01], loss:0.7780541777610779\n",
      "375135232\n",
      "Epoch: 0, batch index: 30, learning rate: [0.01], loss:2.108781099319458\n",
      "375137280\n",
      "Epoch: 0, batch index: 31, learning rate: [0.01], loss:1.221060872077942\n",
      "375135232\n",
      "Epoch: 0, batch index: 32, learning rate: [0.01], loss:1.175909161567688\n",
      "375136768\n",
      "Epoch: 0, batch index: 33, learning rate: [0.01], loss:1.229151964187622\n",
      "375135232\n",
      "Epoch: 0, batch index: 34, learning rate: [0.01], loss:0.9114094376564026\n",
      "375135232\n",
      "Epoch: 0, batch index: 35, learning rate: [0.01], loss:1.5980294942855835\n",
      "375135744\n",
      "Epoch: 0, batch index: 36, learning rate: [0.01], loss:1.0559120178222656\n",
      "375135232\n",
      "Epoch: 0, batch index: 37, learning rate: [0.01], loss:1.2814829349517822\n",
      "375135744\n",
      "Epoch: 0, batch index: 38, learning rate: [0.01], loss:0.8999496698379517\n",
      "375135232\n",
      "Epoch: 0, batch index: 39, learning rate: [0.01], loss:0.5957514047622681\n",
      "375135232\n",
      "Epoch: 0, batch index: 40, learning rate: [0.01], loss:0.9498040676116943\n",
      "375135232\n",
      "Epoch: 0, batch index: 41, learning rate: [0.01], loss:1.4981515407562256\n",
      "375135744\n",
      "Epoch: 0, batch index: 42, learning rate: [0.01], loss:1.511149287223816\n",
      "375135744\n",
      "Epoch: 0, batch index: 43, learning rate: [0.01], loss:0.8388866186141968\n",
      "375135232\n",
      "Epoch: 0, batch index: 44, learning rate: [0.01], loss:0.6436547636985779\n",
      "375135232\n",
      "Epoch: 0, batch index: 45, learning rate: [0.01], loss:1.731491208076477\n",
      "375135232\n",
      "Epoch: 0, batch index: 46, learning rate: [0.01], loss:0.7677342891693115\n",
      "375135232\n",
      "Epoch: 0, batch index: 47, learning rate: [0.01], loss:0.7850202322006226\n",
      "375135232\n",
      "Epoch: 0, batch index: 48, learning rate: [0.01], loss:1.4321341514587402\n",
      "375137280\n",
      "Epoch: 0, batch index: 49, learning rate: [0.01], loss:1.1877859830856323\n",
      "375136256\n",
      "Epoch: 0, batch index: 50, learning rate: [0.01], loss:2.0569100379943848\n",
      "375135232\n",
      "Epoch: 0, batch index: 51, learning rate: [0.01], loss:0.7673976421356201\n",
      "375135232\n",
      "Epoch: 0, batch index: 52, learning rate: [0.01], loss:1.11605703830719\n",
      "375135744\n",
      "Epoch: 0, batch index: 53, learning rate: [0.01], loss:1.234743595123291\n",
      "375135232\n",
      "Epoch: 0, batch index: 54, learning rate: [0.01], loss:1.0397316217422485\n",
      "375136768\n",
      "Epoch: 0, batch index: 55, learning rate: [0.01], loss:2.3229737281799316\n",
      "375135232\n",
      "Epoch: 0, batch index: 56, learning rate: [0.01], loss:1.1026709079742432\n",
      "375135744\n",
      "Epoch: 0, batch index: 57, learning rate: [0.01], loss:1.3608341217041016\n",
      "375135744\n",
      "Epoch: 0, batch index: 58, learning rate: [0.01], loss:0.6948838233947754\n",
      "375135744\n",
      "Epoch: 0, batch index: 59, learning rate: [0.01], loss:1.3964952230453491\n",
      "375136256\n",
      "Epoch: 0, batch index: 60, learning rate: [0.01], loss:1.0663084983825684\n",
      "375135744\n",
      "Epoch: 0, batch index: 61, learning rate: [0.01], loss:0.6132040023803711\n",
      "375135232\n",
      "Epoch: 0, batch index: 62, learning rate: [0.01], loss:1.631394624710083\n",
      "375135232\n",
      "Epoch: 0, batch index: 63, learning rate: [0.01], loss:1.0174616575241089\n",
      "375135232\n",
      "Epoch: 0, batch index: 64, learning rate: [0.01], loss:0.9321191906929016\n",
      "375135744\n",
      "Epoch: 0, batch index: 65, learning rate: [0.01], loss:1.3364189863204956\n",
      "375136256\n",
      "Epoch: 0, batch index: 66, learning rate: [0.01], loss:1.5851569175720215\n",
      "375135744\n",
      "Epoch: 0, batch index: 67, learning rate: [0.01], loss:0.7376148104667664\n",
      "375135232\n",
      "Epoch: 0, batch index: 68, learning rate: [0.01], loss:1.3199747800827026\n",
      "375135232\n",
      "Epoch: 0, batch index: 69, learning rate: [0.01], loss:1.071547508239746\n",
      "375135744\n",
      "Epoch: 0, batch index: 70, learning rate: [0.01], loss:1.0876917839050293\n",
      "375136256\n",
      "Epoch: 0, batch index: 71, learning rate: [0.01], loss:1.0807232856750488\n",
      "375135744\n",
      "Epoch: 0, batch index: 72, learning rate: [0.01], loss:0.9386379718780518\n",
      "375135232\n",
      "Epoch: 0, batch index: 73, learning rate: [0.001], loss:1.3387658596038818\n",
      "375135232\n",
      "Epoch: 0, batch index: 74, learning rate: [0.001], loss:0.9214310050010681\n",
      "375135232\n",
      "Epoch: 0, batch index: 75, learning rate: [0.001], loss:1.8366471529006958\n",
      "375146496\n",
      "Epoch: 0, batch index: 76, learning rate: [0.001], loss:0.8269363045692444\n",
      "375135232\n",
      "Epoch: 0, batch index: 77, learning rate: [0.001], loss:1.1523151397705078\n",
      "375136768\n",
      "Epoch: 0, batch index: 78, learning rate: [0.001], loss:0.9598867893218994\n",
      "375135232\n",
      "Epoch: 0, batch index: 79, learning rate: [0.001], loss:0.923240065574646\n",
      "375135232\n",
      "Epoch: 0, batch index: 80, learning rate: [0.001], loss:1.4684269428253174\n",
      "375135232\n",
      "Epoch: 0, batch index: 81, learning rate: [0.001], loss:1.2788920402526855\n",
      "375135744\n",
      "Epoch: 0, batch index: 82, learning rate: [0.001], loss:0.9773627519607544\n",
      "375135232\n",
      "Epoch: 0, batch index: 83, learning rate: [0.001], loss:1.1458295583724976\n",
      "375137280\n",
      "Epoch: 0, batch index: 84, learning rate: [0.001], loss:0.8892520070075989\n",
      "375135232\n",
      "Epoch: 0, batch index: 85, learning rate: [0.001], loss:1.3344175815582275\n",
      "375135744\n",
      "Epoch: 0, batch index: 86, learning rate: [0.001], loss:0.7172125577926636\n",
      "375135232\n",
      "Epoch: 0, batch index: 87, learning rate: [0.001], loss:0.9788721203804016\n",
      "375136256\n",
      "Epoch: 0, batch index: 88, learning rate: [0.001], loss:1.3247132301330566\n",
      "375135232\n",
      "Epoch: 0, batch index: 89, learning rate: [0.001], loss:0.697229266166687\n",
      "375135744\n",
      "Epoch: 0, batch index: 90, learning rate: [0.001], loss:0.9048793315887451\n",
      "375135232\n",
      "Epoch: 0, batch index: 91, learning rate: [0.001], loss:1.4018495082855225\n",
      "375144448\n",
      "Epoch: 0, batch index: 92, learning rate: [0.001], loss:1.0158793926239014\n",
      "375142400\n",
      "Epoch: 0, batch index: 93, learning rate: [0.001], loss:1.3475568294525146\n",
      "375135232\n",
      "Epoch: 0, batch index: 94, learning rate: [0.001], loss:0.7639228701591492\n",
      "375135232\n",
      "Epoch: 0, batch index: 95, learning rate: [0.001], loss:0.7646048665046692\n",
      "375135232\n",
      "Epoch: 0, batch index: 96, learning rate: [0.001], loss:1.0774070024490356\n",
      "375135744\n",
      "Epoch: 0, batch index: 97, learning rate: [0.001], loss:0.3850134313106537\n",
      "375135232\n",
      "Epoch: 0, batch index: 98, learning rate: [0.001], loss:1.2659368515014648\n",
      "375136768\n",
      "Epoch: 0, batch index: 99, learning rate: [0.001], loss:1.2043434381484985\n",
      "375138816\n",
      "Epoch: 0, batch index: 100, learning rate: [0.001], loss:1.2385739088058472\n",
      "375135744\n",
      "Epoch: 0, batch index: 101, learning rate: [0.001], loss:1.1332014799118042\n",
      "375136768\n",
      "Epoch: 0, batch index: 102, learning rate: [0.001], loss:0.923599362373352\n",
      "375135232\n",
      "Epoch: 0, batch index: 103, learning rate: [0.001], loss:1.2641656398773193\n",
      "375135744\n",
      "Epoch: 0, batch index: 104, learning rate: [0.001], loss:1.0694829225540161\n",
      "375135744\n",
      "Epoch: 0, batch index: 105, learning rate: [0.001], loss:1.3671859502792358\n",
      "375140864\n",
      "Epoch: 0, batch index: 106, learning rate: [0.001], loss:1.0934088230133057\n",
      "375135232\n",
      "Epoch: 0, batch index: 107, learning rate: [0.001], loss:0.7131778597831726\n",
      "375135232\n",
      "Epoch: 0, batch index: 108, learning rate: [0.001], loss:1.0982997417449951\n",
      "375139840\n",
      "Epoch: 0, batch index: 109, learning rate: [0.001], loss:1.0730189085006714\n",
      "375137280\n",
      "Epoch: 0, batch index: 110, learning rate: [0.001], loss:0.7062461376190186\n",
      "375135232\n",
      "Epoch: 0, batch index: 111, learning rate: [0.001], loss:1.300912857055664\n",
      "375137280\n",
      "Epoch: 0, batch index: 112, learning rate: [0.001], loss:1.2127679586410522\n",
      "375137280\n",
      "Epoch: 0, batch index: 113, learning rate: [0.001], loss:1.305153727531433\n",
      "375136768\n",
      "Epoch: 0, batch index: 114, learning rate: [0.001], loss:0.9920734763145447\n",
      "375135232\n",
      "Epoch: 0, batch index: 115, learning rate: [0.001], loss:1.1995790004730225\n",
      "375135232\n",
      "Epoch: 0, batch index: 116, learning rate: [0.001], loss:0.6067713499069214\n",
      "375135232\n",
      "Epoch: 0, batch index: 117, learning rate: [0.001], loss:0.4621683955192566\n",
      "375135232\n",
      "Epoch: 0, batch index: 118, learning rate: [0.001], loss:1.016759991645813\n",
      "375135744\n",
      "Epoch: 0, batch index: 119, learning rate: [0.001], loss:1.2464078664779663\n",
      "375137792\n",
      "Epoch: 0, batch index: 120, learning rate: [0.001], loss:1.0301694869995117\n",
      "375135232\n",
      "Epoch: 0, batch index: 121, learning rate: [0.001], loss:1.1714906692504883\n",
      "375136768\n",
      "Epoch: 0, batch index: 122, learning rate: [0.001], loss:0.6704542636871338\n",
      "375135232\n",
      "Epoch: 0, batch index: 123, learning rate: [0.001], loss:1.2782565355300903\n",
      "375135744\n",
      "Epoch: 0, batch index: 124, learning rate: [0.001], loss:0.9930850863456726\n",
      "375135232\n",
      "Epoch: 0, batch index: 125, learning rate: [0.001], loss:0.7131126523017883\n",
      "375135232\n",
      "Epoch: 0, batch index: 126, learning rate: [0.001], loss:1.5321651697158813\n",
      "375136768\n",
      "Epoch: 0, batch index: 127, learning rate: [0.001], loss:0.9187055826187134\n",
      "375135744\n",
      "Epoch: 0, batch index: 128, learning rate: [0.001], loss:0.6540594100952148\n",
      "375135232\n",
      "Epoch: 0, batch index: 129, learning rate: [0.001], loss:1.1577450037002563\n",
      "375135232\n",
      "Epoch: 0, batch index: 130, learning rate: [0.001], loss:1.175681233406067\n",
      "375135232\n",
      "Epoch: 0, batch index: 131, learning rate: [0.001], loss:0.6134825348854065\n",
      "375135232\n",
      "Epoch: 0, batch index: 132, learning rate: [0.001], loss:0.9911720752716064\n",
      "375135232\n",
      "Epoch: 0, batch index: 133, learning rate: [0.001], loss:1.671696662902832\n",
      "375142400\n",
      "Epoch: 0, batch index: 134, learning rate: [0.001], loss:1.231960415840149\n",
      "375135744\n",
      "Epoch: 0, batch index: 135, learning rate: [0.001], loss:1.043555498123169\n",
      "375136256\n",
      "Epoch: 0, batch index: 136, learning rate: [0.001], loss:0.8323642611503601\n",
      "375135232\n",
      "Epoch: 0, batch index: 137, learning rate: [0.001], loss:1.170924186706543\n",
      "375135232\n",
      "Epoch: 0, batch index: 138, learning rate: [0.001], loss:1.2208538055419922\n",
      "375136256\n",
      "Epoch: 0, batch index: 139, learning rate: [0.001], loss:0.6167852878570557\n",
      "375135232\n",
      "Epoch: 0, batch index: 140, learning rate: [0.001], loss:1.0008435249328613\n",
      "375135232\n",
      "Epoch: 0, batch index: 141, learning rate: [0.001], loss:1.0947790145874023\n",
      "375135744\n",
      "Epoch: 0, batch index: 142, learning rate: [0.001], loss:0.7789813280105591\n",
      "375135232\n",
      "Epoch: 0, batch index: 143, learning rate: [0.001], loss:0.6518366932868958\n",
      "375135232\n",
      "Epoch: 0, batch index: 144, learning rate: [0.001], loss:1.017598271369934\n",
      "375135744\n",
      "Epoch: 0, batch index: 145, learning rate: [0.001], loss:0.9217596650123596\n",
      "375135744\n",
      "Epoch: 0, batch index: 146, learning rate: [0.001], loss:1.0434948205947876\n",
      "375135232\n",
      "Epoch: 0, batch index: 147, learning rate: [0.001], loss:1.5986037254333496\n",
      "375135744\n",
      "Epoch: 0, batch index: 148, learning rate: [0.001], loss:1.0201261043548584\n",
      "375147520\n",
      "Epoch: 0, batch index: 149, learning rate: [0.0001], loss:1.4914767742156982\n",
      "375137792\n",
      "Epoch: 0, batch index: 150, learning rate: [0.0001], loss:1.590755581855774\n",
      "375137280\n",
      "Epoch: 0, batch index: 151, learning rate: [0.0001], loss:0.6669430732727051\n",
      "375135232\n",
      "Epoch: 0, batch index: 152, learning rate: [0.0001], loss:0.7422084808349609\n",
      "375135232\n",
      "Epoch: 0, batch index: 153, learning rate: [0.0001], loss:0.9029660820960999\n",
      "375137280\n",
      "Epoch: 0, batch index: 154, learning rate: [0.0001], loss:1.0504376888275146\n",
      "375135232\n",
      "Epoch: 0, batch index: 155, learning rate: [0.0001], loss:0.863885760307312\n",
      "375135232\n",
      "Epoch: 0, batch index: 156, learning rate: [0.0001], loss:0.7475438714027405\n",
      "375135232\n",
      "Epoch: 0, batch index: 157, learning rate: [0.0001], loss:0.9019909501075745\n",
      "375135744\n",
      "Epoch: 0, batch index: 158, learning rate: [0.0001], loss:1.0485950708389282\n",
      "375135232\n",
      "Epoch: 0, batch index: 159, learning rate: [0.0001], loss:1.1665494441986084\n",
      "375136768\n",
      "Epoch: 0, batch index: 160, learning rate: [0.0001], loss:0.9159483909606934\n",
      "375135232\n",
      "Epoch: 0, batch index: 161, learning rate: [0.0001], loss:0.7195401191711426\n",
      "375135232\n",
      "Epoch: 0, batch index: 162, learning rate: [0.0001], loss:1.3148162364959717\n",
      "375135744\n",
      "Epoch: 0, batch index: 163, learning rate: [0.0001], loss:0.5228942632675171\n",
      "375135232\n",
      "Epoch: 0, batch index: 164, learning rate: [0.0001], loss:0.8189792633056641\n",
      "375135232\n",
      "Epoch: 0, batch index: 165, learning rate: [0.0001], loss:1.420379638671875\n",
      "375136256\n",
      "Epoch: 0, batch index: 166, learning rate: [0.0001], loss:1.1037001609802246\n",
      "375135232\n",
      "Epoch: 0, batch index: 167, learning rate: [0.0001], loss:1.0798866748809814\n",
      "375135744\n",
      "Epoch: 0, batch index: 168, learning rate: [0.0001], loss:0.7392476797103882\n",
      "375135232\n",
      "Epoch: 0, batch index: 169, learning rate: [0.0001], loss:1.0473999977111816\n",
      "375135232\n",
      "Epoch: 0, batch index: 170, learning rate: [0.0001], loss:0.9817454814910889\n",
      "375139328\n",
      "Epoch: 0, batch index: 171, learning rate: [0.0001], loss:1.3341038227081299\n",
      "375137280\n",
      "Epoch: 0, batch index: 172, learning rate: [0.0001], loss:0.9752486944198608\n",
      "375135232\n",
      "Epoch: 0, batch index: 173, learning rate: [0.0001], loss:0.7587109804153442\n",
      "375135232\n",
      "Epoch: 0, batch index: 174, learning rate: [0.0001], loss:0.9293549060821533\n",
      "375135744\n",
      "Epoch: 0, batch index: 175, learning rate: [0.0001], loss:1.139940619468689\n",
      "375136256\n",
      "Epoch: 0, batch index: 176, learning rate: [0.0001], loss:0.9287603497505188\n",
      "375135232\n",
      "Epoch: 0, batch index: 177, learning rate: [0.0001], loss:1.0543630123138428\n",
      "375135232\n",
      "Epoch: 0, batch index: 178, learning rate: [0.0001], loss:0.9208784103393555\n",
      "375138816\n",
      "Epoch: 0, batch index: 179, learning rate: [0.0001], loss:0.8615196943283081\n",
      "375135232\n",
      "Epoch: 0, batch index: 180, learning rate: [0.0001], loss:1.0425209999084473\n",
      "375135232\n",
      "Epoch: 0, batch index: 181, learning rate: [0.0001], loss:0.674016535282135\n",
      "375135232\n",
      "Epoch: 0, batch index: 182, learning rate: [0.0001], loss:1.171095848083496\n",
      "375135232\n",
      "Epoch: 0, batch index: 183, learning rate: [0.0001], loss:1.0985196828842163\n",
      "375135744\n",
      "Epoch: 0, batch index: 184, learning rate: [0.0001], loss:0.5550978779792786\n",
      "375135232\n",
      "Epoch: 0, batch index: 185, learning rate: [0.0001], loss:1.1436538696289062\n",
      "375135744\n",
      "Epoch: 0, batch index: 186, learning rate: [0.0001], loss:1.1691094636917114\n",
      "375135232\n",
      "Epoch: 0, batch index: 187, learning rate: [0.0001], loss:1.1583794355392456\n",
      "375135232\n",
      "Epoch: 0, batch index: 188, learning rate: [0.0001], loss:1.3848562240600586\n",
      "375140352\n",
      "Epoch: 0, batch index: 189, learning rate: [0.0001], loss:1.4243979454040527\n",
      "375137280\n",
      "Epoch: 0, batch index: 190, learning rate: [0.0001], loss:1.1341580152511597\n",
      "375135232\n",
      "Epoch: 0, batch index: 191, learning rate: [0.0001], loss:0.9424149394035339\n",
      "375135744\n",
      "Epoch: 0, batch index: 192, learning rate: [0.0001], loss:1.1891496181488037\n",
      "375135744\n",
      "Epoch: 0, batch index: 193, learning rate: [0.0001], loss:0.7007399797439575\n",
      "375135232\n",
      "Epoch: 0, batch index: 194, learning rate: [0.0001], loss:0.8798894882202148\n",
      "375135744\n",
      "Epoch: 0, batch index: 195, learning rate: [0.0001], loss:1.2840001583099365\n",
      "375136256\n",
      "Epoch: 0, batch index: 196, learning rate: [0.0001], loss:1.3604235649108887\n",
      "375137792\n",
      "Epoch: 0, batch index: 197, learning rate: [0.0001], loss:1.1191117763519287\n",
      "375136256\n",
      "Epoch: 0, batch index: 198, learning rate: [0.0001], loss:1.3255270719528198\n",
      "375135232\n",
      "Epoch: 0, batch index: 199, learning rate: [0.0001], loss:0.6753731966018677\n",
      "375135232\n",
      "Epoch: 0, batch index: 200, learning rate: [0.0001], loss:1.2999308109283447\n",
      "375135232\n",
      "Epoch: 0, batch index: 201, learning rate: [0.0001], loss:0.887030303478241\n",
      "375142912\n",
      "Epoch: 0, batch index: 202, learning rate: [0.0001], loss:0.9815973043441772\n",
      "375136256\n",
      "Epoch: 0, batch index: 203, learning rate: [0.0001], loss:0.9683332443237305\n",
      "375136256\n",
      "Epoch: 0, batch index: 204, learning rate: [0.0001], loss:0.7206577062606812\n",
      "375135232\n",
      "Epoch: 0, batch index: 205, learning rate: [0.0001], loss:0.8631871938705444\n",
      "375135232\n",
      "Epoch: 0, batch index: 206, learning rate: [0.0001], loss:0.7113816738128662\n",
      "375135744\n",
      "Epoch: 0, batch index: 207, learning rate: [1e-05], loss:1.1272214651107788\n",
      "375135744\n",
      "Epoch: 0, batch index: 208, learning rate: [1e-05], loss:1.262411117553711\n",
      "375153152\n",
      "Epoch: 0, batch index: 209, learning rate: [1e-05], loss:1.1977384090423584\n",
      "375135744\n",
      "Epoch: 0, batch index: 210, learning rate: [1e-05], loss:0.46973589062690735\n",
      "375135232\n",
      "Epoch: 0, batch index: 211, learning rate: [1e-05], loss:1.0967930555343628\n",
      "375139328\n",
      "Epoch: 0, batch index: 212, learning rate: [1e-05], loss:0.9099583029747009\n",
      "375135232\n",
      "Epoch: 0, batch index: 213, learning rate: [1e-05], loss:1.5707842111587524\n",
      "375137280\n",
      "Epoch: 0, batch index: 214, learning rate: [1e-05], loss:0.9406963586807251\n",
      "375135232\n",
      "Epoch: 0, batch index: 215, learning rate: [1e-05], loss:0.6264347434043884\n",
      "375135232\n",
      "Epoch: 0, batch index: 216, learning rate: [1e-05], loss:1.0171071290969849\n",
      "375135744\n",
      "Epoch: 0, batch index: 217, learning rate: [1e-05], loss:0.9875752329826355\n",
      "375152128\n",
      "Epoch: 0, batch index: 218, learning rate: [1e-05], loss:0.987923264503479\n",
      "375135232\n",
      "Epoch: 0, batch index: 219, learning rate: [1e-05], loss:0.7625030279159546\n",
      "375135232\n",
      "Epoch: 0, batch index: 220, learning rate: [1e-05], loss:1.2861874103546143\n",
      "375135744\n",
      "Epoch: 0, batch index: 221, learning rate: [1e-05], loss:0.9860255718231201\n",
      "375135744\n",
      "Epoch: 0, batch index: 222, learning rate: [1e-05], loss:1.208094596862793\n",
      "375135744\n",
      "Epoch: 0, batch index: 223, learning rate: [1e-05], loss:0.4507218599319458\n",
      "375135232\n",
      "Epoch: 0, batch index: 224, learning rate: [1e-05], loss:0.8172883987426758\n",
      "375136768\n",
      "Epoch: 0, batch index: 225, learning rate: [1e-05], loss:0.9539740681648254\n",
      "375135232\n",
      "Epoch: 0, batch index: 226, learning rate: [1e-05], loss:1.4983949661254883\n",
      "375140864\n",
      "Epoch: 0, batch index: 227, learning rate: [1e-05], loss:0.7087455987930298\n",
      "375135744\n",
      "Epoch: 0, batch index: 228, learning rate: [1e-05], loss:0.9210675954818726\n",
      "375136256\n",
      "Epoch: 0, batch index: 229, learning rate: [1e-05], loss:1.2522995471954346\n",
      "375135232\n",
      "Epoch: 0, batch index: 230, learning rate: [1e-05], loss:1.2362865209579468\n",
      "375138816\n",
      "Epoch: 0, batch index: 231, learning rate: [1e-05], loss:1.4796044826507568\n",
      "375135744\n",
      "Epoch: 0, batch index: 232, learning rate: [1e-05], loss:1.069528341293335\n",
      "375135744\n",
      "Epoch: 0, batch index: 233, learning rate: [1e-05], loss:1.4388691186904907\n",
      "375136256\n",
      "Epoch: 0, batch index: 234, learning rate: [1e-05], loss:0.8389669060707092\n",
      "375135232\n",
      "Epoch: 0, batch index: 235, learning rate: [1e-05], loss:0.7040279507637024\n",
      "375135232\n",
      "Epoch: 0, batch index: 236, learning rate: [1e-05], loss:1.582840085029602\n",
      "375139328\n",
      "Epoch: 0, batch index: 237, learning rate: [1e-05], loss:1.0565608739852905\n",
      "375135232\n",
      "Epoch: 0, batch index: 238, learning rate: [1e-05], loss:1.3736231327056885\n",
      "375150080\n",
      "Epoch: 0, batch index: 239, learning rate: [1e-05], loss:0.7717139720916748\n",
      "375135232\n",
      "Epoch: 0, batch index: 240, learning rate: [1e-05], loss:0.6580576300621033\n",
      "375135232\n",
      "Epoch: 0, batch index: 241, learning rate: [1e-05], loss:0.6456952095031738\n",
      "375135232\n",
      "Epoch: 0, batch index: 242, learning rate: [1e-05], loss:1.1690531969070435\n",
      "375146496\n",
      "Epoch: 0, batch index: 243, learning rate: [1e-05], loss:0.6764200925827026\n",
      "375135232\n",
      "Epoch: 0, batch index: 244, learning rate: [1e-05], loss:0.9466015100479126\n",
      "375145984\n",
      "Epoch: 0, batch index: 245, learning rate: [1e-05], loss:0.7037796974182129\n",
      "375135232\n",
      "Epoch: 0, batch index: 246, learning rate: [1e-05], loss:0.6677693724632263\n",
      "375135232\n",
      "Epoch: 0, batch index: 247, learning rate: [1e-05], loss:0.9050312638282776\n",
      "375135232\n",
      "Epoch: 0, batch index: 248, learning rate: [1e-05], loss:1.0085556507110596\n",
      "375135232\n",
      "Epoch: 0, batch index: 249, learning rate: [1e-05], loss:1.2198834419250488\n",
      "375139328\n",
      "Epoch: 0, batch index: 250, learning rate: [1e-05], loss:0.8086749911308289\n",
      "375135744\n",
      "Epoch: 0, batch index: 251, learning rate: [1e-05], loss:1.2333892583847046\n",
      "375136768\n",
      "Epoch: 0, batch index: 252, learning rate: [1e-05], loss:1.017512559890747\n",
      "375138816\n",
      "Epoch: 0, batch index: 253, learning rate: [1e-05], loss:0.9528788924217224\n",
      "375139328\n",
      "Epoch: 0, batch index: 254, learning rate: [1e-05], loss:1.0270321369171143\n",
      "375135232\n",
      "Epoch: 0, batch index: 255, learning rate: [1e-05], loss:1.0796682834625244\n",
      "375135232\n",
      "Epoch: 0, batch index: 256, learning rate: [1e-05], loss:0.8533362150192261\n",
      "375135232\n",
      "Epoch: 0, batch index: 257, learning rate: [1e-05], loss:0.9855316877365112\n",
      "375135232\n",
      "Epoch: 0, batch index: 258, learning rate: [1e-05], loss:0.7794473171234131\n",
      "375135232\n",
      "Epoch: 0, batch index: 259, learning rate: [1e-05], loss:1.0852978229522705\n",
      "375136768\n",
      "Epoch: 0, batch index: 260, learning rate: [1e-05], loss:1.1696341037750244\n",
      "375136768\n",
      "Epoch: 0, batch index: 261, learning rate: [1e-05], loss:1.3517595529556274\n",
      "375136256\n",
      "Epoch: 0, batch index: 262, learning rate: [1e-05], loss:0.6928417086601257\n",
      "375135232\n",
      "Epoch: 0, batch index: 263, learning rate: [1e-05], loss:1.3669565916061401\n",
      "375135744\n",
      "Epoch: 0, batch index: 264, learning rate: [1e-05], loss:1.4152376651763916\n",
      "375136768\n",
      "Epoch: 0, batch index: 265, learning rate: [1.0000000000000002e-06], loss:1.0442304611206055\n",
      "375135744\n",
      "Epoch: 0, batch index: 266, learning rate: [1.0000000000000002e-06], loss:0.7563724517822266\n",
      "375135232\n",
      "Epoch: 0, batch index: 267, learning rate: [1.0000000000000002e-06], loss:0.8199017643928528\n",
      "375135232\n",
      "Epoch: 0, batch index: 268, learning rate: [1.0000000000000002e-06], loss:0.4477684199810028\n",
      "375135232\n",
      "Epoch: 0, batch index: 269, learning rate: [1.0000000000000002e-06], loss:1.2135226726531982\n",
      "375140352\n",
      "Epoch: 0, batch index: 270, learning rate: [1.0000000000000002e-06], loss:0.8133931159973145\n",
      "375135232\n",
      "Epoch: 0, batch index: 271, learning rate: [1.0000000000000002e-06], loss:1.0804270505905151\n",
      "375135232\n",
      "Epoch: 0, batch index: 272, learning rate: [1.0000000000000002e-06], loss:1.3042383193969727\n",
      "375138816\n",
      "Epoch: 0, batch index: 273, learning rate: [1.0000000000000002e-06], loss:0.8131808638572693\n",
      "375135232\n",
      "Epoch: 0, batch index: 274, learning rate: [1.0000000000000002e-06], loss:0.9249335527420044\n",
      "375135232\n",
      "Epoch: 0, batch index: 275, learning rate: [1.0000000000000002e-06], loss:0.9423009753227234\n",
      "375135232\n",
      "Epoch: 0, batch index: 276, learning rate: [1.0000000000000002e-06], loss:0.8796519041061401\n",
      "375135744\n",
      "Epoch: 0, batch index: 277, learning rate: [1.0000000000000002e-06], loss:1.2133069038391113\n",
      "375138816\n",
      "Epoch: 0, batch index: 278, learning rate: [1.0000000000000002e-06], loss:0.8187508583068848\n",
      "375135232\n",
      "Epoch: 0, batch index: 279, learning rate: [1.0000000000000002e-06], loss:1.0004417896270752\n",
      "375135232\n",
      "Epoch: 0, batch index: 280, learning rate: [1.0000000000000002e-06], loss:1.1403868198394775\n",
      "375138304\n",
      "Epoch: 0, batch index: 281, learning rate: [1.0000000000000002e-06], loss:0.658646285533905\n",
      "375135232\n",
      "Epoch: 0, batch index: 282, learning rate: [1.0000000000000002e-06], loss:0.5037853717803955\n",
      "375135232\n",
      "Epoch: 0, batch index: 283, learning rate: [1.0000000000000002e-06], loss:0.9297235012054443\n",
      "375135232\n",
      "Epoch: 0, batch index: 284, learning rate: [1.0000000000000002e-06], loss:0.9019845724105835\n",
      "375135744\n",
      "Epoch: 0, batch index: 285, learning rate: [1.0000000000000002e-06], loss:1.106002926826477\n",
      "375136256\n",
      "Epoch: 0, batch index: 286, learning rate: [1.0000000000000002e-06], loss:0.8738293647766113\n",
      "375135232\n",
      "Epoch: 0, batch index: 287, learning rate: [1.0000000000000002e-06], loss:0.99903804063797\n",
      "375135232\n",
      "Epoch: 0, batch index: 288, learning rate: [1.0000000000000002e-06], loss:1.0571985244750977\n",
      "375135232\n",
      "Epoch: 0, batch index: 289, learning rate: [1.0000000000000002e-06], loss:0.832852840423584\n",
      "375135232\n",
      "Epoch: 0, batch index: 290, learning rate: [1.0000000000000002e-06], loss:0.44570961594581604\n",
      "375135232\n",
      "Epoch: 0, batch index: 291, learning rate: [1.0000000000000002e-06], loss:1.484141230583191\n",
      "375135232\n",
      "Epoch: 0, batch index: 292, learning rate: [1.0000000000000002e-06], loss:1.1798391342163086\n",
      "375135744\n",
      "Epoch: 0, batch index: 293, learning rate: [1.0000000000000002e-06], loss:0.8752488493919373\n",
      "375135232\n",
      "Epoch: 0, batch index: 294, learning rate: [1.0000000000000002e-06], loss:0.8553269505500793\n",
      "375135232\n",
      "Epoch: 0, batch index: 295, learning rate: [1.0000000000000002e-06], loss:1.256500005722046\n",
      "375135744\n",
      "Epoch: 0, batch index: 296, learning rate: [1.0000000000000002e-06], loss:1.383720874786377\n",
      "375139840\n",
      "Epoch: 0, batch index: 297, learning rate: [1.0000000000000002e-06], loss:0.9697772860527039\n",
      "375135232\n",
      "Epoch: 0, batch index: 298, learning rate: [1.0000000000000002e-06], loss:1.247770071029663\n",
      "375137280\n",
      "Epoch: 0, batch index: 299, learning rate: [1.0000000000000002e-06], loss:0.9001861810684204\n",
      "375136256\n",
      "Epoch: 0, batch index: 300, learning rate: [1.0000000000000002e-06], loss:0.9902293086051941\n",
      "375135744\n",
      "Epoch: 0, batch index: 301, learning rate: [1.0000000000000002e-06], loss:1.038215160369873\n",
      "375135232\n",
      "Epoch: 0, batch index: 302, learning rate: [1.0000000000000002e-06], loss:0.8513445258140564\n",
      "375135232\n",
      "Epoch: 0, batch index: 303, learning rate: [1.0000000000000002e-06], loss:1.2393286228179932\n",
      "375135232\n",
      "Epoch: 0, batch index: 304, learning rate: [1.0000000000000002e-06], loss:0.8942596912384033\n",
      "375136768\n",
      "Epoch: 0, batch index: 305, learning rate: [1.0000000000000002e-06], loss:0.9219703078269958\n",
      "375135232\n",
      "Epoch: 0, batch index: 306, learning rate: [1.0000000000000002e-06], loss:0.9657153487205505\n",
      "375136768\n",
      "Epoch: 0, batch index: 307, learning rate: [1.0000000000000002e-06], loss:1.679447889328003\n",
      "375140864\n",
      "Epoch: 0, batch index: 308, learning rate: [1.0000000000000002e-06], loss:1.2809600830078125\n",
      "375136768\n",
      "Epoch: 0, batch index: 309, learning rate: [1.0000000000000002e-06], loss:1.2738038301467896\n",
      "375137792\n",
      "Epoch: 0, batch index: 310, learning rate: [1.0000000000000002e-06], loss:1.4056676626205444\n",
      "375140864\n",
      "Epoch: 0, batch index: 311, learning rate: [1.0000000000000002e-06], loss:0.8289144039154053\n",
      "375135232\n",
      "Epoch: 0, batch index: 312, learning rate: [1.0000000000000002e-06], loss:1.0014739036560059\n",
      "375136768\n",
      "Epoch: 0, batch index: 313, learning rate: [1.0000000000000002e-06], loss:1.481482744216919\n",
      "375142400\n",
      "Epoch: 0, batch index: 314, learning rate: [1.0000000000000002e-06], loss:0.9350616931915283\n",
      "375136256\n",
      "Epoch: 0, batch index: 315, learning rate: [1.0000000000000002e-06], loss:0.8958996534347534\n",
      "375135232\n",
      "Epoch: 0, batch index: 316, learning rate: [1.0000000000000002e-06], loss:0.6498396396636963\n",
      "375135744\n",
      "Epoch: 0, batch index: 317, learning rate: [1.0000000000000002e-06], loss:1.1451822519302368\n",
      "375147520\n",
      "Epoch: 0, batch index: 318, learning rate: [1.0000000000000002e-06], loss:1.04109525680542\n",
      "375135744\n",
      "Epoch: 0, batch index: 319, learning rate: [1.0000000000000002e-06], loss:0.5645589232444763\n",
      "375135232\n",
      "Epoch: 0, batch index: 320, learning rate: [1.0000000000000002e-06], loss:1.1583175659179688\n",
      "375143936\n",
      "Epoch: 0, batch index: 321, learning rate: [1.0000000000000002e-06], loss:1.0221645832061768\n",
      "375135744\n",
      "Epoch: 0, batch index: 322, learning rate: [1.0000000000000002e-06], loss:0.7799160480499268\n",
      "375135232\n",
      "Epoch: 0, batch index: 323, learning rate: [1.0000000000000002e-07], loss:1.0432276725769043\n",
      "375135744\n",
      "Epoch: 0, batch index: 324, learning rate: [1.0000000000000002e-07], loss:1.0092566013336182\n",
      "375135232\n",
      "Epoch: 0, batch index: 325, learning rate: [1.0000000000000002e-07], loss:1.110262393951416\n",
      "375135744\n",
      "Epoch: 0, batch index: 326, learning rate: [1.0000000000000002e-07], loss:0.9172261953353882\n",
      "375135232\n",
      "Epoch: 0, batch index: 327, learning rate: [1.0000000000000002e-07], loss:0.9109632968902588\n",
      "375135232\n",
      "Epoch: 0, batch index: 328, learning rate: [1.0000000000000002e-07], loss:1.1989197731018066\n",
      "375136256\n",
      "Epoch: 0, batch index: 329, learning rate: [1.0000000000000002e-07], loss:0.9586269855499268\n",
      "375135232\n",
      "Epoch: 0, batch index: 330, learning rate: [1.0000000000000002e-07], loss:0.913549542427063\n",
      "375135232\n",
      "Epoch: 0, batch index: 331, learning rate: [1.0000000000000002e-07], loss:1.1219289302825928\n",
      "375135744\n",
      "Epoch: 0, batch index: 332, learning rate: [1.0000000000000002e-07], loss:0.8172117471694946\n",
      "375135744\n",
      "Epoch: 0, batch index: 333, learning rate: [1.0000000000000002e-07], loss:0.7913905382156372\n",
      "375135232\n",
      "Epoch: 0, batch index: 334, learning rate: [1.0000000000000002e-07], loss:1.042229413986206\n",
      "375135232\n",
      "Epoch: 0, batch index: 335, learning rate: [1.0000000000000002e-07], loss:1.2231171131134033\n",
      "375136768\n",
      "Epoch: 0, batch index: 336, learning rate: [1.0000000000000002e-07], loss:1.0179646015167236\n",
      "375136256\n",
      "Epoch: 0, batch index: 337, learning rate: [1.0000000000000002e-07], loss:1.6055989265441895\n",
      "375135232\n",
      "Epoch: 0, batch index: 338, learning rate: [1.0000000000000002e-07], loss:0.7191049456596375\n",
      "375135744\n",
      "Epoch: 0, batch index: 339, learning rate: [1.0000000000000002e-07], loss:0.9092983603477478\n",
      "375135232\n",
      "Epoch: 0, batch index: 340, learning rate: [1.0000000000000002e-07], loss:0.8998127579689026\n",
      "375137280\n",
      "Epoch: 0, batch index: 341, learning rate: [1.0000000000000002e-07], loss:1.2013660669326782\n",
      "375136768\n",
      "Epoch: 0, batch index: 342, learning rate: [1.0000000000000002e-07], loss:0.6993129253387451\n",
      "375135232\n",
      "Epoch: 0, batch index: 343, learning rate: [1.0000000000000002e-07], loss:0.7456392049789429\n",
      "375135232\n",
      "Epoch: 0, batch index: 344, learning rate: [1.0000000000000002e-07], loss:1.385758876800537\n",
      "375144448\n",
      "Epoch: 0, batch index: 345, learning rate: [1.0000000000000002e-07], loss:1.1514310836791992\n",
      "375136256\n",
      "Epoch: 0, batch index: 346, learning rate: [1.0000000000000002e-07], loss:1.0576931238174438\n",
      "375142400\n",
      "Epoch: 0, batch index: 347, learning rate: [1.0000000000000002e-07], loss:1.1594929695129395\n",
      "375135232\n",
      "Epoch: 0, batch index: 348, learning rate: [1.0000000000000002e-07], loss:0.7919182777404785\n",
      "375135232\n",
      "Epoch: 0, batch index: 349, learning rate: [1.0000000000000002e-07], loss:1.193756341934204\n",
      "375136768\n",
      "Epoch: 0, batch index: 350, learning rate: [1.0000000000000002e-07], loss:1.3446815013885498\n",
      "375136768\n",
      "Epoch: 0, batch index: 351, learning rate: [1.0000000000000002e-07], loss:1.630403757095337\n",
      "375139328\n",
      "Epoch: 0, batch index: 352, learning rate: [1.0000000000000002e-07], loss:0.989120602607727\n",
      "375135744\n",
      "Epoch: 0, batch index: 353, learning rate: [1.0000000000000002e-07], loss:1.1114158630371094\n",
      "375135232\n",
      "Epoch: 0, batch index: 354, learning rate: [1.0000000000000002e-07], loss:1.0643154382705688\n",
      "375135744\n",
      "Epoch: 0, batch index: 355, learning rate: [1.0000000000000002e-07], loss:1.309545636177063\n",
      "375135232\n",
      "Epoch: 0, batch index: 356, learning rate: [1.0000000000000002e-07], loss:0.8235111236572266\n",
      "375135232\n",
      "Epoch: 0, batch index: 357, learning rate: [1.0000000000000002e-07], loss:0.4268319308757782\n",
      "375135232\n",
      "Epoch: 0, batch index: 358, learning rate: [1.0000000000000002e-07], loss:0.9971790313720703\n",
      "375135232\n",
      "Epoch: 0, batch index: 359, learning rate: [1.0000000000000002e-07], loss:0.9092068076133728\n",
      "375135744\n",
      "Epoch: 0, batch index: 360, learning rate: [1.0000000000000002e-07], loss:0.9866268634796143\n",
      "375135232\n",
      "Epoch: 0, batch index: 361, learning rate: [1.0000000000000002e-07], loss:0.6886298656463623\n",
      "375135744\n",
      "Epoch: 0, batch index: 362, learning rate: [1.0000000000000002e-07], loss:1.1551250219345093\n",
      "375135232\n",
      "Epoch: 0, batch index: 363, learning rate: [1.0000000000000002e-07], loss:0.9798117876052856\n",
      "375135232\n",
      "Epoch: 0, batch index: 364, learning rate: [1.0000000000000002e-07], loss:1.0875983238220215\n",
      "375135232\n",
      "Epoch: 0, batch index: 365, learning rate: [1.0000000000000002e-07], loss:1.1844559907913208\n",
      "375135232\n",
      "Epoch: 0, batch index: 366, learning rate: [1.0000000000000002e-07], loss:0.7877392768859863\n",
      "375135744\n",
      "Epoch: 0, batch index: 367, learning rate: [1.0000000000000002e-07], loss:1.2957947254180908\n",
      "375135744\n",
      "Epoch: 0, batch index: 368, learning rate: [1.0000000000000002e-07], loss:0.9308829307556152\n",
      "375135744\n",
      "Epoch: 0, batch index: 369, learning rate: [1.0000000000000002e-07], loss:1.3530919551849365\n",
      "375153664\n",
      "Epoch: 0, batch index: 370, learning rate: [1.0000000000000002e-07], loss:1.1553902626037598\n",
      "375137792\n",
      "Epoch: 0, batch index: 371, learning rate: [1.0000000000000002e-07], loss:1.0061898231506348\n",
      "375140352\n",
      "Epoch: 0, batch index: 372, learning rate: [1.0000000000000002e-07], loss:1.2035882472991943\n",
      "375135232\n",
      "Epoch: 0, batch index: 373, learning rate: [1.0000000000000002e-07], loss:0.9598431587219238\n",
      "375135744\n",
      "Epoch: 0, batch index: 374, learning rate: [1.0000000000000002e-07], loss:0.45491135120391846\n",
      "375135232\n",
      "Epoch: 0, batch index: 375, learning rate: [1.0000000000000002e-07], loss:1.191719651222229\n",
      "375135744\n",
      "Epoch: 0, batch index: 376, learning rate: [1.0000000000000002e-07], loss:0.546614944934845\n",
      "375135232\n",
      "Epoch: 0, batch index: 377, learning rate: [1.0000000000000002e-07], loss:0.4924209713935852\n",
      "375135232\n",
      "Epoch: 0, batch index: 378, learning rate: [1.0000000000000002e-07], loss:2.7203080654144287\n",
      "375137280\n",
      "Epoch: 0, batch index: 379, learning rate: [1.0000000000000002e-07], loss:1.5545921325683594\n",
      "375138816\n",
      "Epoch: 0, batch index: 380, learning rate: [1.0000000000000002e-07], loss:0.821846067905426\n",
      "375135232\n",
      "Epoch: 0, batch index: 381, learning rate: [1.0000000000000004e-08], loss:1.2925851345062256\n",
      "375135744\n",
      "Epoch: 0, batch index: 382, learning rate: [1.0000000000000004e-08], loss:0.8133141398429871\n",
      "375135232\n",
      "Epoch: 0, batch index: 383, learning rate: [1.0000000000000004e-08], loss:1.342411756515503\n",
      "375140864\n",
      "Epoch: 0, batch index: 384, learning rate: [1.0000000000000004e-08], loss:1.083139419555664\n",
      "375135232\n",
      "Epoch: 0, batch index: 385, learning rate: [1.0000000000000004e-08], loss:0.9811466932296753\n",
      "375136256\n",
      "Epoch: 0, batch index: 386, learning rate: [1.0000000000000004e-08], loss:0.9469538927078247\n",
      "375135232\n",
      "Epoch: 0, batch index: 387, learning rate: [1.0000000000000004e-08], loss:0.7737585306167603\n",
      "375135232\n",
      "Epoch: 0, batch index: 388, learning rate: [1.0000000000000004e-08], loss:1.1215107440948486\n",
      "375139328\n",
      "Epoch: 0, batch index: 389, learning rate: [1.0000000000000004e-08], loss:1.0517525672912598\n",
      "375135744\n",
      "Epoch: 0, batch index: 390, learning rate: [1.0000000000000004e-08], loss:1.0931963920593262\n",
      "375135744\n",
      "Epoch: 0, batch index: 391, learning rate: [1.0000000000000004e-08], loss:0.8487012386322021\n",
      "375135744\n",
      "Epoch: 0, batch index: 392, learning rate: [1.0000000000000004e-08], loss:1.027293086051941\n",
      "375135232\n",
      "Epoch: 0, batch index: 393, learning rate: [1.0000000000000004e-08], loss:0.7622395753860474\n",
      "375135232\n",
      "Epoch: 0, batch index: 394, learning rate: [1.0000000000000004e-08], loss:0.8531381487846375\n",
      "375140864\n",
      "Epoch: 0, batch index: 395, learning rate: [1.0000000000000004e-08], loss:0.6522701382637024\n",
      "375135744\n",
      "Epoch: 0, batch index: 396, learning rate: [1.0000000000000004e-08], loss:1.1417839527130127\n",
      "375135744\n",
      "Epoch: 0, batch index: 397, learning rate: [1.0000000000000004e-08], loss:1.3722741603851318\n",
      "375137792\n",
      "Epoch: 0, batch index: 398, learning rate: [1.0000000000000004e-08], loss:1.0735336542129517\n",
      "375138816\n",
      "Epoch: 0, batch index: 399, learning rate: [1.0000000000000004e-08], loss:1.1075254678726196\n",
      "375138816\n",
      "Epoch: 0, batch index: 400, learning rate: [1.0000000000000004e-08], loss:1.248141884803772\n",
      "375135744\n",
      "Epoch: 0, batch index: 401, learning rate: [1.0000000000000004e-08], loss:0.8847473859786987\n",
      "375135232\n",
      "Epoch: 0, batch index: 402, learning rate: [1.0000000000000004e-08], loss:1.009885549545288\n",
      "375135744\n",
      "Epoch: 0, batch index: 403, learning rate: [1.0000000000000004e-08], loss:1.056498646736145\n",
      "375140864\n",
      "Epoch: 0, batch index: 404, learning rate: [1.0000000000000004e-08], loss:0.8641352653503418\n",
      "375135232\n",
      "Epoch: 0, batch index: 405, learning rate: [1.0000000000000004e-08], loss:0.8620307445526123\n",
      "375135232\n",
      "Epoch: 0, batch index: 406, learning rate: [1.0000000000000004e-08], loss:0.5571238398551941\n",
      "375135232\n",
      "Epoch: 0, batch index: 407, learning rate: [1.0000000000000004e-08], loss:0.9813676476478577\n",
      "375135744\n",
      "Epoch: 0, batch index: 408, learning rate: [1.0000000000000004e-08], loss:1.7003586292266846\n",
      "375136768\n",
      "Epoch: 0, batch index: 409, learning rate: [1.0000000000000004e-08], loss:0.9462447166442871\n",
      "375135744\n",
      "Epoch: 0, batch index: 410, learning rate: [1.0000000000000004e-08], loss:1.227806806564331\n",
      "375137280\n",
      "Epoch: 0, batch index: 411, learning rate: [1.0000000000000004e-08], loss:0.956615686416626\n",
      "375135744\n",
      "Epoch: 0, batch index: 412, learning rate: [1.0000000000000004e-08], loss:0.8690731525421143\n",
      "375135232\n",
      "Epoch: 0, batch index: 413, learning rate: [1.0000000000000004e-08], loss:0.7257257699966431\n",
      "375135232\n",
      "Epoch: 0, batch index: 414, learning rate: [1.0000000000000004e-08], loss:0.732144832611084\n",
      "375135744\n",
      "Epoch: 0, batch index: 415, learning rate: [1.0000000000000004e-08], loss:0.9613174796104431\n",
      "375135744\n",
      "Epoch: 0, batch index: 416, learning rate: [1.0000000000000004e-08], loss:0.961276650428772\n",
      "375135232\n",
      "Epoch: 0, batch index: 417, learning rate: [1.0000000000000004e-08], loss:1.1566755771636963\n",
      "375137280\n",
      "Epoch: 0, batch index: 418, learning rate: [1.0000000000000004e-08], loss:0.75443434715271\n",
      "375139328\n",
      "Epoch: 0, batch index: 419, learning rate: [1.0000000000000004e-08], loss:0.95399409532547\n",
      "375135744\n",
      "Epoch: 0, batch index: 420, learning rate: [1.0000000000000004e-08], loss:1.385377287864685\n",
      "375137280\n",
      "Epoch: 0, batch index: 421, learning rate: [1.0000000000000004e-08], loss:0.6536065340042114\n",
      "375135232\n",
      "Epoch: 0, batch index: 422, learning rate: [1.0000000000000004e-08], loss:0.45872819423675537\n",
      "375135232\n",
      "Epoch: 0, batch index: 423, learning rate: [1.0000000000000004e-08], loss:1.4689311981201172\n",
      "375138816\n",
      "Epoch: 0, batch index: 424, learning rate: [1.0000000000000004e-08], loss:0.8886459469795227\n",
      "375137280\n",
      "Epoch: 0, batch index: 425, learning rate: [1.0000000000000004e-08], loss:1.6517441272735596\n",
      "375149568\n",
      "Epoch: 0, batch index: 426, learning rate: [1.0000000000000004e-08], loss:0.9674433469772339\n",
      "375136256\n",
      "Epoch: 0, batch index: 427, learning rate: [1.0000000000000004e-08], loss:1.1994602680206299\n",
      "375137280\n",
      "Epoch: 0, batch index: 428, learning rate: [1.0000000000000004e-08], loss:1.2712173461914062\n",
      "375135744\n",
      "Epoch: 0, batch index: 429, learning rate: [1.0000000000000004e-08], loss:0.8281139731407166\n",
      "375135232\n",
      "Epoch: 0, batch index: 430, learning rate: [1.0000000000000004e-08], loss:0.9328542351722717\n",
      "375135232\n",
      "Epoch: 0, batch index: 431, learning rate: [1.0000000000000004e-08], loss:0.9373481273651123\n",
      "375135232\n",
      "Epoch: 0, batch index: 432, learning rate: [1.0000000000000004e-08], loss:1.853863000869751\n",
      "375145984\n",
      "Epoch: 0, batch index: 433, learning rate: [1.0000000000000004e-08], loss:0.9930615425109863\n",
      "375135232\n",
      "Epoch: 0, batch index: 434, learning rate: [1.0000000000000004e-08], loss:1.3493146896362305\n",
      "375135744\n",
      "Epoch: 0, batch index: 435, learning rate: [1.0000000000000004e-08], loss:0.8443252444267273\n",
      "375135232\n",
      "Epoch: 0, batch index: 436, learning rate: [1.0000000000000004e-08], loss:1.0993391275405884\n",
      "375135232\n",
      "Epoch: 0, batch index: 437, learning rate: [1.0000000000000004e-08], loss:0.43489858508110046\n",
      "375135232\n",
      "Epoch: 0, batch index: 438, learning rate: [1.0000000000000004e-08], loss:1.2857903242111206\n",
      "375135744\n",
      "Epoch: 0, batch index: 439, learning rate: [1.0000000000000005e-09], loss:1.2295117378234863\n",
      "375135232\n",
      "Epoch: 0, batch index: 440, learning rate: [1.0000000000000005e-09], loss:1.2411339282989502\n",
      "375139328\n",
      "Epoch: 0, batch index: 441, learning rate: [1.0000000000000005e-09], loss:0.9233724474906921\n",
      "375150080\n",
      "Epoch: 0, batch index: 442, learning rate: [1.0000000000000005e-09], loss:0.876196026802063\n",
      "375140864\n",
      "Epoch: 0, batch index: 443, learning rate: [1.0000000000000005e-09], loss:0.9893641471862793\n",
      "375138816\n",
      "Epoch: 0, batch index: 444, learning rate: [1.0000000000000005e-09], loss:0.7291160225868225\n",
      "375135232\n",
      "Epoch: 0, batch index: 445, learning rate: [1.0000000000000005e-09], loss:0.7065444588661194\n",
      "375135232\n",
      "Epoch: 0, batch index: 446, learning rate: [1.0000000000000005e-09], loss:0.8875848650932312\n",
      "375135232\n",
      "Epoch: 0, batch index: 447, learning rate: [1.0000000000000005e-09], loss:1.1983728408813477\n",
      "375138816\n",
      "Epoch: 0, batch index: 448, learning rate: [1.0000000000000005e-09], loss:1.19915771484375\n",
      "375135744\n",
      "Epoch: 0, batch index: 449, learning rate: [1.0000000000000005e-09], loss:1.4870429039001465\n",
      "375140864\n",
      "Epoch: 0, batch index: 450, learning rate: [1.0000000000000005e-09], loss:0.9748842120170593\n",
      "375135744\n",
      "Epoch: 0, batch index: 451, learning rate: [1.0000000000000005e-09], loss:1.3192520141601562\n",
      "375135232\n",
      "Epoch: 0, batch index: 452, learning rate: [1.0000000000000005e-09], loss:1.256255030632019\n",
      "375135744\n",
      "Epoch: 0, batch index: 453, learning rate: [1.0000000000000005e-09], loss:0.8544160723686218\n",
      "375138816\n",
      "Epoch: 0, batch index: 454, learning rate: [1.0000000000000005e-09], loss:0.7261186838150024\n",
      "375135232\n",
      "Epoch: 0, batch index: 455, learning rate: [1.0000000000000005e-09], loss:1.2270766496658325\n",
      "375139328\n",
      "Epoch: 0, batch index: 456, learning rate: [1.0000000000000005e-09], loss:1.2226747274398804\n",
      "375135232\n",
      "Epoch: 0, batch index: 457, learning rate: [1.0000000000000005e-09], loss:1.0033726692199707\n",
      "375135744\n",
      "Epoch: 0, batch index: 458, learning rate: [1.0000000000000005e-09], loss:1.0391193628311157\n",
      "375135232\n",
      "Epoch: 0, batch index: 459, learning rate: [1.0000000000000005e-09], loss:1.1518863439559937\n",
      "375136768\n",
      "Epoch: 0, batch index: 460, learning rate: [1.0000000000000005e-09], loss:1.0077403783798218\n",
      "375135232\n",
      "Epoch: 0, batch index: 461, learning rate: [1.0000000000000005e-09], loss:1.1169054508209229\n",
      "375135232\n",
      "Epoch: 0, batch index: 462, learning rate: [1.0000000000000005e-09], loss:1.202884554862976\n",
      "375135744\n",
      "Epoch: 0, batch index: 463, learning rate: [1.0000000000000005e-09], loss:1.031810998916626\n",
      "375139328\n",
      "Epoch: 0, batch index: 464, learning rate: [1.0000000000000005e-09], loss:0.9696961045265198\n",
      "375135744\n",
      "Epoch: 0, batch index: 465, learning rate: [1.0000000000000005e-09], loss:0.48165982961654663\n",
      "375135232\n",
      "Epoch: 0, batch index: 466, learning rate: [1.0000000000000005e-09], loss:1.5321803092956543\n",
      "375136256\n",
      "Epoch: 0, batch index: 467, learning rate: [1.0000000000000005e-09], loss:1.122380018234253\n",
      "375136256\n",
      "Epoch: 0, batch index: 468, learning rate: [1.0000000000000005e-09], loss:0.9197379350662231\n",
      "375135232\n",
      "Epoch: 0, batch index: 469, learning rate: [1.0000000000000005e-09], loss:1.0581293106079102\n",
      "375135744\n",
      "Epoch: 0, batch index: 470, learning rate: [1.0000000000000005e-09], loss:0.7603908777236938\n",
      "375135232\n",
      "Epoch: 0, batch index: 471, learning rate: [1.0000000000000005e-09], loss:0.9254755973815918\n",
      "375135232\n",
      "Epoch: 0, batch index: 472, learning rate: [1.0000000000000005e-09], loss:0.6803361177444458\n",
      "375135232\n",
      "Epoch: 0, batch index: 473, learning rate: [1.0000000000000005e-09], loss:1.138331651687622\n",
      "375135744\n",
      "Epoch: 0, batch index: 474, learning rate: [1.0000000000000005e-09], loss:0.8726197481155396\n",
      "375135232\n",
      "Epoch: 0, batch index: 475, learning rate: [1.0000000000000005e-09], loss:1.1243175268173218\n",
      "375135744\n",
      "Epoch: 0, batch index: 476, learning rate: [1.0000000000000005e-09], loss:0.9765247106552124\n",
      "375135232\n",
      "Epoch: 0, batch index: 477, learning rate: [1.0000000000000005e-09], loss:0.5929805040359497\n",
      "375135232\n",
      "Epoch: 0, batch index: 478, learning rate: [1.0000000000000005e-09], loss:0.8821322321891785\n",
      "375135232\n",
      "Epoch: 0, batch index: 479, learning rate: [1.0000000000000005e-09], loss:1.1920872926712036\n",
      "375135232\n",
      "Epoch: 0, batch index: 480, learning rate: [1.0000000000000005e-09], loss:0.7869365215301514\n",
      "375135232\n",
      "Epoch: 0, batch index: 481, learning rate: [1.0000000000000005e-09], loss:1.265098214149475\n",
      "375135744\n",
      "Epoch: 0, batch index: 482, learning rate: [1.0000000000000005e-09], loss:0.8502221703529358\n",
      "375135232\n",
      "Epoch: 0, batch index: 483, learning rate: [1.0000000000000005e-09], loss:1.502536416053772\n",
      "375136256\n",
      "Epoch: 0, batch index: 484, learning rate: [1.0000000000000005e-09], loss:1.4006705284118652\n",
      "375139328\n",
      "Epoch: 0, batch index: 485, learning rate: [1.0000000000000005e-09], loss:0.9933953881263733\n",
      "375135232\n",
      "Epoch: 0, batch index: 486, learning rate: [1.0000000000000005e-09], loss:0.7773231267929077\n",
      "375135744\n",
      "Epoch: 0, batch index: 487, learning rate: [1.0000000000000005e-09], loss:1.6494861841201782\n",
      "375138816\n",
      "Epoch: 0, batch index: 488, learning rate: [1.0000000000000005e-09], loss:0.6805544495582581\n",
      "375135232\n",
      "Epoch: 0, batch index: 489, learning rate: [1.0000000000000005e-09], loss:0.9699329733848572\n",
      "375136256\n",
      "Epoch: 0, batch index: 490, learning rate: [1.0000000000000005e-09], loss:0.8062907457351685\n",
      "375135744\n",
      "Epoch: 0, batch index: 491, learning rate: [1.0000000000000005e-09], loss:0.6736293435096741\n",
      "375135232\n",
      "Epoch: 0, batch index: 492, learning rate: [1.0000000000000005e-09], loss:1.5013571977615356\n",
      "375135744\n",
      "Epoch: 0, batch index: 493, learning rate: [1.0000000000000005e-09], loss:0.9479827880859375\n",
      "375136768\n",
      "Epoch: 0, batch index: 494, learning rate: [1.0000000000000005e-09], loss:1.2318121194839478\n",
      "375136768\n",
      "Epoch: 0, batch index: 495, learning rate: [1.0000000000000005e-09], loss:0.9843771457672119\n",
      "375135232\n",
      "Epoch: 0, batch index: 496, learning rate: [1.0000000000000005e-09], loss:0.7917267680168152\n",
      "375135232\n",
      "Epoch: 0, batch index: 497, learning rate: [1.0000000000000006e-10], loss:0.7954469919204712\n",
      "375135232\n",
      "Epoch: 0, batch index: 498, learning rate: [1.0000000000000006e-10], loss:0.8293096423149109\n",
      "375146496\n",
      "Epoch: 0, batch index: 499, learning rate: [1.0000000000000006e-10], loss:0.8414915800094604\n",
      "375137280\n",
      "Epoch: 0, batch index: 500, learning rate: [1.0000000000000006e-10], loss:1.33893883228302\n",
      "375135232\n",
      "Epoch: 0, batch index: 501, learning rate: [1.0000000000000006e-10], loss:1.3001039028167725\n",
      "375137280\n",
      "Epoch: 0, batch index: 502, learning rate: [1.0000000000000006e-10], loss:1.1411550045013428\n",
      "375136256\n",
      "Epoch: 0, batch index: 503, learning rate: [1.0000000000000006e-10], loss:0.8752446174621582\n",
      "375135744\n",
      "Epoch: 0, batch index: 504, learning rate: [1.0000000000000006e-10], loss:0.7561378479003906\n",
      "375135744\n",
      "Epoch: 0, batch index: 505, learning rate: [1.0000000000000006e-10], loss:0.9410398602485657\n",
      "375135744\n",
      "Epoch: 0, batch index: 506, learning rate: [1.0000000000000006e-10], loss:1.1480287313461304\n",
      "375135232\n",
      "Epoch: 0, batch index: 507, learning rate: [1.0000000000000006e-10], loss:1.548737645149231\n",
      "375138304\n",
      "Epoch: 0, batch index: 508, learning rate: [1.0000000000000006e-10], loss:0.7512003779411316\n",
      "375140352\n",
      "Epoch: 0, batch index: 509, learning rate: [1.0000000000000006e-10], loss:0.9421705007553101\n",
      "375135744\n",
      "Epoch: 0, batch index: 510, learning rate: [1.0000000000000006e-10], loss:1.2760679721832275\n",
      "375140352\n",
      "Epoch: 0, batch index: 511, learning rate: [1.0000000000000006e-10], loss:1.1332085132598877\n",
      "375135232\n",
      "Epoch: 0, batch index: 512, learning rate: [1.0000000000000006e-10], loss:0.8885663151741028\n",
      "375135744\n",
      "Epoch: 0, batch index: 513, learning rate: [1.0000000000000006e-10], loss:0.9088977575302124\n",
      "375135232\n",
      "Epoch: 0, batch index: 514, learning rate: [1.0000000000000006e-10], loss:1.0185717344284058\n",
      "375135232\n",
      "Epoch: 0, batch index: 515, learning rate: [1.0000000000000006e-10], loss:1.1722371578216553\n",
      "375135232\n",
      "Epoch: 0, batch index: 516, learning rate: [1.0000000000000006e-10], loss:0.5010936260223389\n",
      "375135232\n",
      "Epoch: 0, batch index: 517, learning rate: [1.0000000000000006e-10], loss:0.8584908246994019\n",
      "375135232\n",
      "Epoch: 0, batch index: 518, learning rate: [1.0000000000000006e-10], loss:0.9663087129592896\n",
      "375135232\n",
      "Epoch: 0, batch index: 519, learning rate: [1.0000000000000006e-10], loss:0.9448448419570923\n",
      "375135232\n",
      "Epoch: 0, batch index: 520, learning rate: [1.0000000000000006e-10], loss:0.6782857179641724\n",
      "375135232\n",
      "Epoch: 0, batch index: 521, learning rate: [1.0000000000000006e-10], loss:0.4611315429210663\n",
      "375135232\n",
      "Epoch: 0, batch index: 522, learning rate: [1.0000000000000006e-10], loss:0.7241947650909424\n",
      "375135232\n",
      "Epoch: 0, batch index: 523, learning rate: [1.0000000000000006e-10], loss:0.9753302335739136\n",
      "365300736\n",
      "Loading best model\n"
     ]
    }
   ],
   "source": [
    "train(model, epochs=1, learning_rate=0.01, train_loader=train_loader, validation_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'weights/best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2697, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_from_valiadation(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "** (xviewer:33534): CRITICAL **: 14:00:00.854: xviewer_image_get_file: assertion 'XVIEWER_IS_IMAGE (img)' failed\n",
      "\n",
      "(xviewer:33534): GLib-GIO-CRITICAL **: 14:00:00.854: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "** (xviewer:33534): CRITICAL **: 14:00:00.984: xviewer_image_get_file: assertion 'XVIEWER_IS_IMAGE (img)' failed\n",
      "\n",
      "(xviewer:33534): GLib-GIO-CRITICAL **: 14:00:00.984: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "** (xviewer:33534): CRITICAL **: 14:00:01.062: xviewer_list_store_get_pos_by_image: assertion 'XVIEWER_IS_IMAGE (image)' failed\n",
      "\n",
      "** (xviewer:33534): CRITICAL **: 14:00:01.062: xviewer_image_get_file: assertion 'XVIEWER_IS_IMAGE (img)' failed\n",
      "\n",
      "(xviewer:33534): GLib-GIO-CRITICAL **: 14:00:01.062: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "** (xviewer:33534): CRITICAL **: 14:00:01.287: xviewer_image_get_file: assertion 'XVIEWER_IS_IMAGE (img)' failed\n",
      "\n",
      "(xviewer:33534): GLib-GIO-CRITICAL **: 14:00:01.287: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = next(iter(valid_loader))\n",
    "\n",
    "features, labels = batch\n",
    "predictions = model([img.to(device) for img in features])\n",
    "\n",
    "for img, box, label in zip(features, predictions, labels):\n",
    "    with torch.no_grad():\n",
    "        formatted_boxes = box['boxes']\n",
    "        show_bounding_boxes(img, box['boxes'])\n",
    "        show_bounding_boxes(img, label['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
