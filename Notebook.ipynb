{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-30 11:38:15.270979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-30 11:38:15.905747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, tv_tensors\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/train/_annotations.coco.json') as file:\n",
    "    my_dict = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n",
      "[{'id': 0, 'image_id': 0, 'category_id': 3, 'bbox': [259, 49, 4.8, 9.6], 'area': 46.08, 'segmentation': [[264, 48.8, 259.2, 48.8, 259.2, 58.4, 264, 58.4, 264, 48.8]], 'iscrowd': 0}, {'id': 1, 'image_id': 0, 'category_id': 3, 'bbox': [284, 630, 4.8, 8.8], 'area': 42.24, 'segmentation': [[288.8, 630.4, 284, 630.4, 284, 639.2, 288.8, 639.2, 288.8, 630.4]], 'iscrowd': 0}]\n",
      "[{'id': 0, 'license': 1, 'file_name': 'P2491__1-0__1200___1764_png_jpg.rf.00342c6c14ae53b3bfadd7995643e1bc.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-12-20T13:55:24+00:00'}, {'id': 1, 'license': 1, 'file_name': '4f833867-273e-4d73-8bc3-cb2d9ceb54ef_0_0_jpg.rf.000c42e196c096916dfe7c0744d06e12.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-12-20T13:55:24+00:00'}]\n",
      "[{'id': 1, 'name': 'Aircraft'}, {'id': 2, 'name': 'ship'}, {'id': 3, 'name': 'vehicle'}]\n"
     ]
    }
   ],
   "source": [
    "print(my_dict.keys())\n",
    "print(my_dict['annotations'][0:2])\n",
    "print(my_dict['images'][0:2])\n",
    "print(my_dict['categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AerialViewDataset(Dataset):\n",
    "    def __init__(self, root_folder) -> None:\n",
    "        super().__init__()\n",
    "        self.root_folder = root_folder\n",
    "\n",
    "        with open(f'{self.root_folder}/_annotations.coco.json') as file:\n",
    "            self.raw_dictionary = json.load(file)\n",
    "\n",
    "        self.list_of_image_dictionaries = self.raw_dictionary['images']\n",
    "\n",
    "        # Adds all the annotations to the dictionary containing a particular images' details under the key 'list_of_annotations'\n",
    "        for dict in self.list_of_image_dictionaries:\n",
    "            dict['list_of_annotations'] = []\n",
    "            for annotation in self.raw_dictionary['annotations']:\n",
    "                if annotation['image_id'] == dict['id']:\n",
    "                    dict['list_of_annotations'].append(annotation)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple containing 'Image' and 'Target'\n",
    "        \"\"\"\n",
    "        image_path = f'{self.root_folder}' + self.raw_dictionary['images'][index]['file_name']\n",
    "        transformer = transforms.PILToTensor()\n",
    "        with Image.open(image_path) as pil_image:\n",
    "            image = transformer(pil_image)\n",
    "            image = image.float()\n",
    "        target = {}\n",
    "        target['area'] = []\n",
    "        target['boxes'] = []\n",
    "        target['image_id'] = torch.tensor(self.list_of_image_dictionaries[index]['id'])\n",
    "        target['labels'] = []\n",
    "\n",
    "        for annotation in self.list_of_image_dictionaries[index]['list_of_annotations']:\n",
    "            target['area'].append(annotation['area'])\n",
    "            target['boxes'].append(annotation['bbox'])\n",
    "            target['labels'].append(annotation['category_id'])\n",
    "        \n",
    "        target['area'] = torch.Tensor(target['area']).float()\n",
    "\n",
    "        # Convert the boxes attribute to tensors and then format it to xyxy from xywh\n",
    "        target['boxes'] = tv_tensors.BoundingBoxes(target['boxes'], format='xywh', canvas_size=(640, 640))\n",
    "        target['boxes'] = torchvision.ops.box_convert(target['boxes'],  in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "        target['labels'] = torch.Tensor(target['labels']).long()\n",
    "        \n",
    "        target['area'].to(device)\n",
    "        target['boxes'].to(device)\n",
    "        target['image_id'].to(device)\n",
    "        target['labels'].to(device)\n",
    "\n",
    "        return (image, target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_dictionary['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AerialViewDataset('Datasets/train/')\n",
    "valid_dataset = AerialViewDataset('Datasets/valid/')\n",
    "test_dataset = AerialViewDataset('Datasets/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': tensor([44.8000, 33.2800, 35.8400, 40.9600, 38.4000]),\n",
       " 'boxes': tensor([[293.0000, 293.0000, 297.0000, 304.2000],\n",
       "         [299.0000, 109.0000, 302.2000, 119.4000],\n",
       "         [298.0000,  60.0000, 301.2000,  71.2000],\n",
       "         [322.0000, 147.0000, 325.2000, 159.8000],\n",
       "         [318.0000, 154.0000, 321.2000, 166.0000]]),\n",
       " 'image_id': tensor(1),\n",
       " 'labels': tensor([3, 3, 3, 3, 3])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bounding_boxes(image:Image, bounding_boxes:tv_tensors.BoundingBoxes):\n",
    "    transformer = transforms.PILToTensor()\n",
    "    image_tensor = transformer(image)\n",
    "    annotated_image_tensor = torchvision.utils.draw_bounding_boxes(image_tensor, bounding_boxes, colors='green')\n",
    "    transforms.functional.to_pil_image(annotated_image_tensor).show()\n",
    "    \n",
    "def show_bounding_boxes_from_image_id(image_id:int, root_path:str = 'Datasets/train/', list_of_images:list = my_dict['images'], list_of_annotations:list = my_dict['annotations']):\n",
    "    raw_box_coords = []\n",
    "    for annotation in list_of_annotations:\n",
    "        if annotation['image_id'] == image_id:\n",
    "            raw_box_coords.append(annotation['bbox'])\n",
    "    \n",
    "    tensored_box_coords = tv_tensors.BoundingBoxes(raw_box_coords, format='xywh', canvas_size=(640, 640))\n",
    "    formatted_box_coords = torchvision.ops.box_convert(tensored_box_coords, in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "    image_path = f'{root_path}' + list_of_images[image_id]['file_name']\n",
    "\n",
    "    with Image.open(image_path) as image:\n",
    "        show_bounding_boxes(image, formatted_box_coords)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bounding_boxes(transforms.functional.to_pil_image(test_dataset[161][0]), test_dataset[161][1]['boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dir(model)\n",
    "#model.modules\n",
    "num_classes = 4 \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss_classifier': tensor(107.7279, device='cuda:0', grad_fn=<NllLossBackward0>), 'loss_box_reg': tensor(4.8898, device='cuda:0', grad_fn=<DivBackward0>), 'loss_objectness': tensor(38.3661, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), 'loss_rpn_box_reg': tensor(104.8354, device='cuda:0', grad_fn=<DivBackward0>)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = test_dataset[0]\n",
    "features = [test_dataset[0][0], test_dataset[1][0]]\n",
    "targets = [test_dataset[0][1], test_dataset[1][1]]\n",
    "\n",
    "features = [img.to(device) for img in features]\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "print(model(features, targets))\n",
    "model.eval()\n",
    "#print(model(features, targets))\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop. Takes in a model, the training and validation data loaders, the number of epochs and the initial learning rate\n",
    "def train(model, train_loader, validation_loader, epochs = 10, learning_rate = 1, model_name:str = \"My Model\"):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set the optimiser to be an instance of the stochastic gradient descent class\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimiser = torch.optim.SGD(parameters, lr=learning_rate)\n",
    "\n",
    "    # Define a learning rate scheduler as an instance of the ReduceLROnPlateau class\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=50, cooldown=7, eps=1e-20)\n",
    "\n",
    "    # Writer will be used to track model performance with TensorBoard\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Keep track of the number of batches to plot model performace against\n",
    "    batch_index = 0\n",
    "    \n",
    "    # Prints an validation score\n",
    "    #print(f\"Initial validation accuracy score{accuracy_score_from_valiadation(model, validation_loader)}\")\n",
    "\n",
    "    # Create a dictionary to store the best model parameters\n",
    "    #best_model_parameters = {'Epoch':-1, 'Accuracy':0, 'Parameters':model.state_dict()}\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Within each epoch, we pass through the entire training data in batches indexed by batch\n",
    "        for batch in train_loader:\n",
    "            # Loads features and labels into device for performance improvements\n",
    "            features, labels = batch\n",
    "            model.train()\n",
    "            \n",
    "            features = list(img.to(device) for img in features)\n",
    "\n",
    "            print([img.device for img in features])\n",
    "\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "\n",
    "            loss_dict = model(features, labels)\n",
    "\n",
    "            # Calculate the loss via cross_entropy\n",
    "            loss = loss_dict['loss_objectness']\n",
    "\n",
    "            # Create the grad attributes\n",
    "            loss.backward() \n",
    "\n",
    "            # Clip the loss value so it doesn't become Nan\n",
    "            torch.nn.utils.clip_grad_norm_(parameters, 4)\n",
    "\n",
    "            # Print the performance\n",
    "            print(f\"Epoch: {epoch}, batch index: {batch_index}, learning rate: {scheduler.get_last_lr()}, loss:{loss.item()}\")\n",
    "\n",
    "            # Perform one step of stochastic gradient descent\n",
    "            optimiser.step()\n",
    "\n",
    "            # Zero the gradients (Apparently set_to_none=True imporves performace)\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Feed the loss amount into the learning rate scheduler to decide the next learning rate\n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "            # Write the performance to the TensorBoard plot\n",
    "            writer.add_scalar('loss', loss.item(), batch_index)\n",
    "\n",
    "            # Increment the batch index\n",
    "            batch_index += 1\n",
    "        \n",
    "        # Print the validation loss\n",
    "        \"\"\"\n",
    "        print('Calculating validation accuracy')\n",
    "        accuracy = accuracy_score_from_valiadation(model, validation_loader)\n",
    "        print(f\"Epoch {epoch}, validation accuracy score{accuracy}\")\n",
    "        \"\"\"\n",
    "        # Check if the model has the best perfomrance and save the parameters to 'best_model.pt'\n",
    "        \"\"\"\n",
    "        if accuracy > best_model_parameters['Accuracy']:\n",
    "            best_model_parameters['Epoch'] = epoch\n",
    "            best_model_parameters['Accuracy'] = loss.item()\n",
    "            best_model_parameters['Parameters'] = model.state_dict()\n",
    "            torch.save(model.state_dict(), f'model_evaluation/weights/{model_name}_best_model.pt')\n",
    "        \"\"\"\n",
    "    print('Loading best model')\n",
    "    \n",
    "    #Update model parameters with the best model parameters:\n",
    "    #model.load_state_dict(best_model_parameters['Parameters'])\n",
    "    #print(f'The best model has validation accuracy {accuracy_score_from_valiadation(model, validation_loader)}')\n",
    "\n",
    "def accuracy_score_from_valiadation(model, validation_loader):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy using the WHOLE of the validation dataset.\n",
    "    \"\"\"\n",
    "    predictions = torch.zeros(0).to(device)\n",
    "    labels = torch.zeros(0).to(device)\n",
    "\n",
    "    for batch_index, batch in enumerate(validation_loader):\n",
    "        features, target = batch\n",
    "        target.to(device)\n",
    "        if torch.is_tensor(features):\n",
    "            features = features.to(device)\n",
    "        model.to(device)\n",
    "        target = target.to(device)\n",
    "        predictions = torch.cat((predictions, model(features).max(dim = 1).indices))\n",
    "        labels = torch.cat((labels, target))\n",
    "    \n",
    "    accuracy_score = torch.sum(predictions == labels) / len(predictions)\n",
    "\n",
    "    return accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 0, learning rate: [0.01], loss:147.78604125976562\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 1, learning rate: [0.01], loss:361.88671875\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 2, learning rate: [0.01], loss:83.92477416992188\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 3, learning rate: [0.01], loss:238.52716064453125\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 4, learning rate: [0.01], loss:34.35378646850586\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 5, learning rate: [0.01], loss:248.3220977783203\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 6, learning rate: [0.01], loss:17.6578369140625\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 7, learning rate: [0.01], loss:67.62201690673828\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 8, learning rate: [0.01], loss:46.55528259277344\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 9, learning rate: [0.01], loss:66.66574096679688\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 10, learning rate: [0.01], loss:24.39940643310547\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 11, learning rate: [0.01], loss:62.861515045166016\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 12, learning rate: [0.01], loss:13.420402526855469\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 13, learning rate: [0.01], loss:85.60488891601562\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 14, learning rate: [0.01], loss:9.741082191467285\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 15, learning rate: [0.01], loss:8.524982452392578\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 16, learning rate: [0.01], loss:7.941279888153076\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 17, learning rate: [0.01], loss:4.684805870056152\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 18, learning rate: [0.01], loss:4.285924911499023\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 19, learning rate: [0.01], loss:21.51802635192871\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 20, learning rate: [0.01], loss:5.119786262512207\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 21, learning rate: [0.01], loss:6.3412299156188965\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 22, learning rate: [0.01], loss:8.084528923034668\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 23, learning rate: [0.01], loss:6.611966133117676\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 24, learning rate: [0.01], loss:5.768054008483887\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 25, learning rate: [0.01], loss:1.6890504360198975\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 26, learning rate: [0.01], loss:2.376832962036133\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 27, learning rate: [0.01], loss:13.731058120727539\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 28, learning rate: [0.01], loss:1.5552880764007568\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 29, learning rate: [0.01], loss:17.950408935546875\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 30, learning rate: [0.01], loss:1.6403172016143799\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 31, learning rate: [0.01], loss:7.701394081115723\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 32, learning rate: [0.01], loss:2.5776114463806152\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 33, learning rate: [0.01], loss:1.2256423234939575\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 34, learning rate: [0.01], loss:1.1996421813964844\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 35, learning rate: [0.01], loss:3.2706003189086914\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 36, learning rate: [0.01], loss:2.0619099140167236\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 37, learning rate: [0.01], loss:1.0591304302215576\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 38, learning rate: [0.01], loss:0.867871105670929\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 39, learning rate: [0.01], loss:0.7105280160903931\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 40, learning rate: [0.01], loss:1.0322339534759521\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 41, learning rate: [0.01], loss:9.849871635437012\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 42, learning rate: [0.01], loss:1.1774213314056396\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 43, learning rate: [0.01], loss:0.7691388726234436\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 44, learning rate: [0.01], loss:1.9710915088653564\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 45, learning rate: [0.01], loss:0.6870291233062744\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 46, learning rate: [0.01], loss:1.1080193519592285\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 47, learning rate: [0.01], loss:0.5643293857574463\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 48, learning rate: [0.01], loss:9.064661026000977\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 49, learning rate: [0.01], loss:0.7935183048248291\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 50, learning rate: [0.01], loss:0.6486446857452393\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 51, learning rate: [0.01], loss:0.876905083656311\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 52, learning rate: [0.01], loss:0.7205651998519897\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 53, learning rate: [0.01], loss:0.8050285577774048\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 54, learning rate: [0.01], loss:0.7774751782417297\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 55, learning rate: [0.01], loss:1.8014452457427979\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 56, learning rate: [0.01], loss:0.7429543733596802\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 57, learning rate: [0.01], loss:0.7275111079216003\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 58, learning rate: [0.01], loss:0.7569790482521057\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 59, learning rate: [0.01], loss:1.046291470527649\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 60, learning rate: [0.01], loss:0.7140399813652039\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 61, learning rate: [0.01], loss:1.3979188203811646\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 62, learning rate: [0.01], loss:0.7323156595230103\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 63, learning rate: [0.01], loss:1.5420434474945068\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 64, learning rate: [0.01], loss:1.6444692611694336\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 65, learning rate: [0.01], loss:0.6846921443939209\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 66, learning rate: [0.01], loss:0.6867438554763794\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 67, learning rate: [0.01], loss:1.5577418804168701\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 68, learning rate: [0.01], loss:0.7326991558074951\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 69, learning rate: [0.01], loss:0.7767938375473022\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 70, learning rate: [0.01], loss:0.6848355531692505\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 71, learning rate: [0.01], loss:1.415656566619873\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 72, learning rate: [0.01], loss:0.7985038757324219\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 73, learning rate: [0.01], loss:0.7168281078338623\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 74, learning rate: [0.01], loss:0.5410633087158203\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 75, learning rate: [0.01], loss:12.573118209838867\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 76, learning rate: [0.01], loss:1.453295111656189\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 77, learning rate: [0.01], loss:0.7190258502960205\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 78, learning rate: [0.01], loss:0.7379589080810547\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 79, learning rate: [0.01], loss:0.7003995180130005\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 80, learning rate: [0.01], loss:0.6492753028869629\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 81, learning rate: [0.01], loss:0.7025493383407593\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 82, learning rate: [0.01], loss:0.8604094982147217\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 83, learning rate: [0.01], loss:0.6932276487350464\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 84, learning rate: [0.01], loss:0.6965047121047974\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 85, learning rate: [0.01], loss:0.7481541037559509\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 86, learning rate: [0.01], loss:0.70135897397995\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 87, learning rate: [0.01], loss:0.7275968790054321\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 88, learning rate: [0.01], loss:0.6756330132484436\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 89, learning rate: [0.01], loss:2.678664445877075\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 90, learning rate: [0.01], loss:0.9747015237808228\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 91, learning rate: [0.01], loss:0.7127335071563721\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 92, learning rate: [0.01], loss:0.7124533653259277\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 93, learning rate: [0.01], loss:0.8861224055290222\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 94, learning rate: [0.01], loss:0.700797438621521\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 95, learning rate: [0.01], loss:0.9133919477462769\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 96, learning rate: [0.01], loss:1.0370664596557617\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 97, learning rate: [0.01], loss:0.933978796005249\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 98, learning rate: [0.01], loss:0.7329416275024414\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 99, learning rate: [0.01], loss:0.6946542263031006\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 100, learning rate: [0.01], loss:0.7140514254570007\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 101, learning rate: [0.01], loss:1.7257888317108154\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 102, learning rate: [0.01], loss:0.7059189677238464\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 103, learning rate: [0.01], loss:0.699235200881958\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 104, learning rate: [0.01], loss:0.7076506018638611\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n",
      "Epoch: 0, batch index: 105, learning rate: [0.01], loss:0.7081549763679504\n",
      "[device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0), device(type='cuda', index=0)]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 684.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, validation_loader, epochs, learning_rate, model_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m([img\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m features])\n\u001b[1;32m     38\u001b[0m labels \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[0;32m---> 41\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Calculate the loss via cross_entropy\u001b[39;00m\n\u001b[1;32m     44\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_objectness\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:379\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 379\u001b[0m labels, matched_gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_targets_to_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m regression_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[1;32m    381\u001b[0m loss_objectness, loss_rpn_box_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[1;32m    382\u001b[0m     objectness, pred_bbox_deltas, labels, regression_targets\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:208\u001b[0m, in \u001b[0;36mRegionProposalNetwork.assign_targets_to_anchors\u001b[0;34m(self, anchors, targets)\u001b[0m\n\u001b[1;32m    206\u001b[0m     labels_per_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((anchors_per_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     match_quality_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors_per_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_matcher(match_quality_matrix)\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# get the targets corresponding GT for each proposal\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# NB: need to clamp the indices because we can have a single\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# GT in the image, and matched_idxs can be -2, which goes\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# out of bounds\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/ops/boxes.py:287\u001b[0m, in \u001b[0;36mbox_iou\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    286\u001b[0m     _log_api_usage_once(box_iou)\n\u001b[0;32m--> 287\u001b[0m inter, union \u001b[38;5;241m=\u001b[39m \u001b[43m_box_inter_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m iou \u001b[38;5;241m=\u001b[39m inter \u001b[38;5;241m/\u001b[39m union\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iou\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/ops/boxes.py:263\u001b[0m, in \u001b[0;36m_box_inter_union\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    260\u001b[0m lt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, :\u001b[38;5;241m2\u001b[39m], boxes2[:, :\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[1;32m    261\u001b[0m rb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m2\u001b[39m:], boxes2[:, \u001b[38;5;241m2\u001b[39m:])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m wh \u001b[38;5;241m=\u001b[39m \u001b[43m_upcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[1;32m    264\u001b[0m inter \u001b[38;5;241m=\u001b[39m wh[:, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m wh[:, :, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# [N,M]\u001b[39;00m\n\u001b[1;32m    266\u001b[0m union \u001b[38;5;241m=\u001b[39m area1[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m area2 \u001b[38;5;241m-\u001b[39m inter\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 684.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "train(model, epochs=1, learning_rate=0.01, train_loader=train_loader, validation_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
