{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, tv_tensors\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from typing import Union,TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datasets/train/_annotations.coco.json') as file:\n",
    "    my_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['info', 'licenses', 'categories', 'images', 'annotations'])\n",
      "[{'id': 0, 'image_id': 0, 'category_id': 3, 'bbox': [259, 49, 4.8, 9.6], 'area': 46.08, 'segmentation': [[264, 48.8, 259.2, 48.8, 259.2, 58.4, 264, 58.4, 264, 48.8]], 'iscrowd': 0}, {'id': 1, 'image_id': 0, 'category_id': 3, 'bbox': [284, 630, 4.8, 8.8], 'area': 42.24, 'segmentation': [[288.8, 630.4, 284, 630.4, 284, 639.2, 288.8, 639.2, 288.8, 630.4]], 'iscrowd': 0}]\n",
      "[{'id': 0, 'license': 1, 'file_name': 'P2491__1-0__1200___1764_png_jpg.rf.00342c6c14ae53b3bfadd7995643e1bc.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-12-20T13:55:24+00:00'}, {'id': 1, 'license': 1, 'file_name': '4f833867-273e-4d73-8bc3-cb2d9ceb54ef_0_0_jpg.rf.000c42e196c096916dfe7c0744d06e12.jpg', 'height': 640, 'width': 640, 'date_captured': '2023-12-20T13:55:24+00:00'}]\n",
      "[{'id': 1, 'name': 'Aircraft'}, {'id': 2, 'name': 'ship'}, {'id': 3, 'name': 'vehicle'}]\n"
     ]
    }
   ],
   "source": [
    "print(my_dict.keys())\n",
    "print(my_dict['annotations'][0:2])\n",
    "print(my_dict['images'][0:2])\n",
    "print(my_dict['categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AerialViewDataset(Dataset):\n",
    "    def __init__(self, root_folder) -> None:\n",
    "        super().__init__()\n",
    "        self.root_folder = root_folder\n",
    "\n",
    "        with open(f'{self.root_folder}/_annotations.coco.json') as file:\n",
    "            self.raw_dictionary = json.load(file)\n",
    "\n",
    "        self.list_of_image_dictionaries = self.raw_dictionary['images']\n",
    "\n",
    "        # Adds all the annotations to the dictionary containing a particular images' details under the key 'list_of_annotations'\n",
    "        for dict in self.list_of_image_dictionaries:\n",
    "            dict['list_of_annotations'] = []\n",
    "            for annotation in self.raw_dictionary['annotations']:\n",
    "                if annotation['image_id'] == dict['id']:\n",
    "                    dict['list_of_annotations'].append(annotation)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a tuple containing 'Image' and 'Target'\n",
    "        \"\"\"\n",
    "        image_path = f'{self.root_folder}' + self.raw_dictionary['images'][index]['file_name']\n",
    "        transformer = transforms.PILToTensor()\n",
    "        with Image.open(image_path) as pil_image:\n",
    "            image = transformer(pil_image)\n",
    "            image = image.float()\n",
    "        target = {}\n",
    "        target['area'] = []\n",
    "        target['boxes'] = []\n",
    "        target['image_id'] = torch.tensor(self.list_of_image_dictionaries[index]['id'])\n",
    "        target['labels'] = []\n",
    "\n",
    "        for annotation in self.list_of_image_dictionaries[index]['list_of_annotations']:\n",
    "            target['area'].append(annotation['area'])\n",
    "            target['boxes'].append(annotation['bbox'])\n",
    "            target['labels'].append(annotation['category_id'])\n",
    "        \n",
    "        target['area'] = torch.Tensor(target['area']).float()\n",
    "\n",
    "        # Convert the boxes attribute to tensors and then format it to xyxy from xywh\n",
    "        target['boxes'] = tv_tensors.BoundingBoxes(target['boxes'], format='xywh', canvas_size=(640, 640))\n",
    "        target['boxes'] = torchvision.ops.box_convert(target['boxes'],  in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "        target['labels'] = torch.Tensor(target['labels']).long()\n",
    "\n",
    "        return (image, target)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.raw_dictionary['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AerialViewDataset('Datasets/train/')\n",
    "valid_dataset = AerialViewDataset('Datasets/valid/')\n",
    "test_dataset = AerialViewDataset('Datasets/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'area': tensor([44.8000, 33.2800, 35.8400, 40.9600, 38.4000]),\n",
       " 'boxes': tensor([[293.0000, 293.0000, 297.0000, 304.2000],\n",
       "         [299.0000, 109.0000, 302.2000, 119.4000],\n",
       "         [298.0000,  60.0000, 301.2000,  71.2000],\n",
       "         [322.0000, 147.0000, 325.2000, 159.8000],\n",
       "         [318.0000, 154.0000, 321.2000, 166.0000]]),\n",
       " 'image_id': tensor(1),\n",
       " 'labels': tensor([3, 3, 3, 3, 3])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_bounding_boxes(image:Union[Image.Image, torch.tensor], bounding_boxes:tv_tensors.BoundingBoxes):\n",
    "    if type(image) == Image.Image:\n",
    "        transformer = transforms.PILToTensor()\n",
    "        image = transformer(image)\n",
    "    image = image.byte()\n",
    "    annotated_image_tensor = torchvision.utils.draw_bounding_boxes(image, bounding_boxes, colors='green')\n",
    "    transforms.functional.to_pil_image(annotated_image_tensor).show()\n",
    "    \n",
    "def show_bounding_boxes_from_image_id(image_id:int, root_path:str = 'Datasets/train/', list_of_images:list = my_dict['images'], list_of_annotations:list = my_dict['annotations']):\n",
    "    raw_box_coords = []\n",
    "    for annotation in list_of_annotations:\n",
    "        if annotation['image_id'] == image_id:\n",
    "            raw_box_coords.append(annotation['bbox'])\n",
    "    \n",
    "    tensored_box_coords = tv_tensors.BoundingBoxes(raw_box_coords, format='xywh', canvas_size=(640, 640))\n",
    "    formatted_box_coords = torchvision.ops.box_convert(tensored_box_coords, in_fmt='xywh', out_fmt='xyxy')\n",
    "\n",
    "    image_path = f'{root_path}' + list_of_images[image_id]['file_name']\n",
    "\n",
    "    with Image.open(image_path) as image:\n",
    "        show_bounding_boxes(image, formatted_box_coords)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_bounding_boxes(test_dataset[161][0], test_dataset[161][1]['boxes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30338/833195210.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_parameters = torch.load('weights/best_model.pt', map_location=torch.device(device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dir(model)\n",
    "#model.modules\n",
    "num_classes = 4 \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model_parameters = torch.load('weights/best_model.pt', map_location=torch.device(device))\n",
    "model.load_state_dict(model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop. Takes in a model, the training and validation data loaders, the number of epochs and the initial learning rate\n",
    "def train(model, train_loader, validation_loader, epochs = 10, learning_rate = 1, model_name:str = \"My Model\"):\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Set the optimiser to be an instance of the stochastic gradient descent class\n",
    "    parameters = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimiser = torch.optim.SGD(parameters, lr=learning_rate)\n",
    "\n",
    "    # Define a learning rate scheduler as an instance of the ReduceLROnPlateau class\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, mode='min', patience=50, cooldown=7, eps=1e-20)\n",
    "\n",
    "    # Writer will be used to track model performance with TensorBoard\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Keep track of the number of batches to plot model performace against\n",
    "    batch_index = 0\n",
    "    \n",
    "    # Prints an validation score\n",
    "    print(f\"Initial validation accuracy score{accuracy_score_from_valiadation(model, validation_loader)}\")\n",
    "\n",
    "    # Create a dictionary to store the best model parameters\n",
    "    #best_model_parameters = {'Epoch':-1, 'Accuracy':0, 'Parameters':model.state_dict()}\n",
    "\n",
    "    # Loop over the number of epochs\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Within each epoch, we pass through the entire training data in batches indexed by batch\n",
    "        for batch in train_loader:\n",
    "            # Loads features and labels into device for performance improvements\n",
    "            features, labels = batch\n",
    "\n",
    "            model.train()\n",
    "            \n",
    "            features = list(img.to(device) for img in features)\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            loss_dict = model(features, labels)\n",
    "\n",
    "            # Calculate the loss via cross_entropy\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Create the grad attributes\n",
    "            loss.backward() \n",
    "\n",
    "            # Clip the loss value so it doesn't become Nan\n",
    "            torch.nn.utils.clip_grad_norm_(parameters, 4)\n",
    "\n",
    "            # Print the performance\n",
    "            print(f\"Epoch: {epoch}, batch index: {batch_index}, learning rate: {scheduler.get_last_lr()}, loss:{loss.item()}\")\n",
    "\n",
    "            # Perform one step of stochastic gradient descent\n",
    "            optimiser.step()\n",
    "\n",
    "            # Zero the gradients (Apparently set_to_none=True imporves performace)\n",
    "            optimiser.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Feed the loss amount into the learning rate scheduler to decide the next learning rate\n",
    "            scheduler.step(loss.item())\n",
    "\n",
    "            # Write the performance to the TensorBoard plot\n",
    "            writer.add_scalar('loss', loss.item(), batch_index)\n",
    "\n",
    "            # Increment the batch index\n",
    "            batch_index += 1\n",
    "\n",
    "            print(torch.cuda.memory_allocated())\n",
    "            torch.cuda.memory_summary()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Print the validation loss\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Check if the model has the best perfomrance and save the parameters to 'best_model.pt'\n",
    "        \"\"\"\n",
    "        torch.save(model.state_dict(), f'model_evaluation/weights/{model_name}_best_model.pt')\n",
    "        \"\"\"\n",
    "    print('Loading best model')\n",
    "    \n",
    "    #Update model parameters with the best model parameters:\n",
    "    #model.load_state_dict(best_model_parameters['Parameters'])\n",
    "    #print(f'The best model has validation accuracy {accuracy_score_from_valiadation(model, validation_loader)}')\n",
    "\n",
    "def accuracy_score_from_valiadation(model, validation_loader):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy using the WHOLE of the validation dataset.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        losses = torch.zeros(0).to(device)\n",
    "\n",
    "        for batch_index, batch in enumerate(validation_loader):\n",
    "\n",
    "            features, labels = batch\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            features = list(img.to(device) for img in features)\n",
    "            labels = [{k: v.to(device) for k, v in t.items()} for t in labels]\n",
    "\n",
    "            model.to(device)\n",
    "\n",
    "            loss_dict = model(features, labels)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            losses = torch.cat((losses, loss.view(1)))\n",
    "\n",
    "        accuracy_score = torch.sum(losses) / len(losses)\n",
    "\n",
    "        return accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial validation accuracy score1.0118811130523682\n",
      "Epoch: 0, batch index: 0, learning rate: [0.01], loss:1.084619402885437\n",
      "2059663360\n",
      "Epoch: 0, batch index: 1, learning rate: [0.01], loss:1.147750973701477\n",
      "2059663360\n",
      "Epoch: 0, batch index: 2, learning rate: [0.01], loss:2.239684581756592\n",
      "2059674624\n",
      "Epoch: 0, batch index: 3, learning rate: [0.01], loss:1.4452155828475952\n",
      "2059665408\n",
      "Epoch: 0, batch index: 4, learning rate: [0.01], loss:2.5612142086029053\n",
      "2059663360\n",
      "Epoch: 0, batch index: 5, learning rate: [0.01], loss:1.2864947319030762\n",
      "2059666944\n",
      "Epoch: 0, batch index: 6, learning rate: [0.01], loss:1.3928345441818237\n",
      "2059663872\n",
      "Epoch: 0, batch index: 7, learning rate: [0.01], loss:1.3097221851348877\n",
      "2059663872\n",
      "Epoch: 0, batch index: 8, learning rate: [0.01], loss:1.5944314002990723\n",
      "2059663872\n",
      "Epoch: 0, batch index: 9, learning rate: [0.01], loss:1.1857914924621582\n",
      "2059663872\n",
      "Epoch: 0, batch index: 10, learning rate: [0.01], loss:0.9302355647087097\n",
      "2059663360\n",
      "Epoch: 0, batch index: 11, learning rate: [0.01], loss:1.3740973472595215\n",
      "2059663360\n",
      "Epoch: 0, batch index: 12, learning rate: [0.01], loss:0.7923120856285095\n",
      "2059663360\n",
      "Epoch: 0, batch index: 13, learning rate: [0.01], loss:0.7888102531433105\n",
      "2059677696\n",
      "Epoch: 0, batch index: 14, learning rate: [0.01], loss:0.8194864392280579\n",
      "2059663360\n",
      "Epoch: 0, batch index: 15, learning rate: [0.01], loss:2.0827488899230957\n",
      "2059665408\n",
      "Epoch: 0, batch index: 16, learning rate: [0.01], loss:0.9066265821456909\n",
      "2059663360\n",
      "Epoch: 0, batch index: 17, learning rate: [0.01], loss:0.5383090972900391\n",
      "2059663360\n",
      "Epoch: 0, batch index: 18, learning rate: [0.01], loss:0.9160338044166565\n",
      "2059663360\n",
      "Epoch: 0, batch index: 19, learning rate: [0.01], loss:0.6032543182373047\n",
      "2059663360\n",
      "Epoch: 0, batch index: 20, learning rate: [0.01], loss:2.0708975791931152\n",
      "2059663872\n",
      "Epoch: 0, batch index: 21, learning rate: [0.01], loss:0.9411027431488037\n",
      "2059663360\n",
      "Epoch: 0, batch index: 22, learning rate: [0.01], loss:1.1967226266860962\n",
      "2059663872\n",
      "Epoch: 0, batch index: 23, learning rate: [0.01], loss:1.2919588088989258\n",
      "2059666944\n",
      "Epoch: 0, batch index: 24, learning rate: [0.01], loss:1.5482854843139648\n",
      "2059664896\n",
      "Epoch: 0, batch index: 25, learning rate: [0.01], loss:1.6194353103637695\n",
      "2059665920\n",
      "Epoch: 0, batch index: 26, learning rate: [0.01], loss:1.8334962129592896\n",
      "2059667456\n",
      "Epoch: 0, batch index: 27, learning rate: [0.01], loss:1.219964861869812\n",
      "2059663872\n",
      "Epoch: 0, batch index: 28, learning rate: [0.01], loss:0.6264366507530212\n",
      "2059663360\n",
      "Epoch: 0, batch index: 29, learning rate: [0.01], loss:0.9396928548812866\n",
      "2059663360\n",
      "Epoch: 0, batch index: 30, learning rate: [0.01], loss:1.1239488124847412\n",
      "2059666944\n",
      "Epoch: 0, batch index: 31, learning rate: [0.01], loss:1.1670622825622559\n",
      "2059670528\n",
      "Epoch: 0, batch index: 32, learning rate: [0.01], loss:0.8648150563240051\n",
      "2059663360\n",
      "Epoch: 0, batch index: 33, learning rate: [0.01], loss:1.3875640630722046\n",
      "2059663360\n",
      "Epoch: 0, batch index: 34, learning rate: [0.01], loss:0.7258479595184326\n",
      "2059663872\n",
      "Epoch: 0, batch index: 35, learning rate: [0.01], loss:0.8627723455429077\n",
      "2059663360\n",
      "Epoch: 0, batch index: 36, learning rate: [0.01], loss:2.2002673149108887\n",
      "2059674624\n",
      "Epoch: 0, batch index: 37, learning rate: [0.01], loss:0.9762008190155029\n",
      "2059663360\n",
      "Epoch: 0, batch index: 38, learning rate: [0.01], loss:1.4409066438674927\n",
      "2059666944\n",
      "Epoch: 0, batch index: 39, learning rate: [0.01], loss:1.4683758020401\n",
      "2059663872\n",
      "Epoch: 0, batch index: 40, learning rate: [0.01], loss:1.1640996932983398\n",
      "2059663360\n",
      "Epoch: 0, batch index: 41, learning rate: [0.01], loss:1.4770983457565308\n",
      "2059667456\n",
      "Epoch: 0, batch index: 42, learning rate: [0.01], loss:1.315830945968628\n",
      "2059663360\n",
      "Epoch: 0, batch index: 43, learning rate: [0.01], loss:0.4980124831199646\n",
      "2059663360\n",
      "Epoch: 0, batch index: 44, learning rate: [0.01], loss:1.530768632888794\n",
      "2059671040\n",
      "Epoch: 0, batch index: 45, learning rate: [0.01], loss:0.88626629114151\n",
      "2059663360\n",
      "Epoch: 0, batch index: 46, learning rate: [0.01], loss:1.090157389640808\n",
      "2059663872\n",
      "Epoch: 0, batch index: 47, learning rate: [0.01], loss:1.043473243713379\n",
      "2059663360\n",
      "Epoch: 0, batch index: 48, learning rate: [0.01], loss:0.9067407846450806\n",
      "2059663872\n",
      "Epoch: 0, batch index: 49, learning rate: [0.01], loss:0.9006389379501343\n",
      "2059663872\n",
      "Epoch: 0, batch index: 50, learning rate: [0.01], loss:0.7162457704544067\n",
      "2059663872\n",
      "Epoch: 0, batch index: 51, learning rate: [0.01], loss:0.8998963832855225\n",
      "2059664896\n",
      "Epoch: 0, batch index: 52, learning rate: [0.01], loss:0.7391955256462097\n",
      "2059663360\n",
      "Epoch: 0, batch index: 53, learning rate: [0.01], loss:4.855838775634766\n",
      "2059663360\n",
      "Epoch: 0, batch index: 54, learning rate: [0.01], loss:1.4913837909698486\n",
      "2059664896\n",
      "Epoch: 0, batch index: 55, learning rate: [0.01], loss:0.7194948196411133\n",
      "2059663360\n",
      "Epoch: 0, batch index: 56, learning rate: [0.01], loss:1.1662778854370117\n",
      "2059679744\n",
      "Epoch: 0, batch index: 57, learning rate: [0.01], loss:1.2190016508102417\n",
      "2059664896\n",
      "Epoch: 0, batch index: 58, learning rate: [0.01], loss:1.1551096439361572\n",
      "2059663360\n",
      "Epoch: 0, batch index: 59, learning rate: [0.01], loss:1.0635850429534912\n",
      "2059663360\n",
      "Epoch: 0, batch index: 60, learning rate: [0.01], loss:1.492051362991333\n",
      "2059664384\n",
      "Epoch: 0, batch index: 61, learning rate: [0.01], loss:0.5738818049430847\n",
      "2059663360\n",
      "Epoch: 0, batch index: 62, learning rate: [0.01], loss:1.648834466934204\n",
      "2059663872\n",
      "Epoch: 0, batch index: 63, learning rate: [0.01], loss:1.4751930236816406\n",
      "2059663872\n",
      "Epoch: 0, batch index: 64, learning rate: [0.01], loss:1.593675136566162\n",
      "2059664896\n",
      "Epoch: 0, batch index: 65, learning rate: [0.01], loss:1.0874693393707275\n",
      "2059663360\n",
      "Epoch: 0, batch index: 66, learning rate: [0.01], loss:0.9055585265159607\n",
      "2059663360\n",
      "Epoch: 0, batch index: 67, learning rate: [0.01], loss:1.3211772441864014\n",
      "2059663360\n",
      "Epoch: 0, batch index: 68, learning rate: [0.01], loss:0.9083439707756042\n",
      "2059663360\n",
      "Epoch: 0, batch index: 69, learning rate: [0.01], loss:2.3437860012054443\n",
      "2059663360\n",
      "Epoch: 0, batch index: 70, learning rate: [0.01], loss:0.8852163553237915\n",
      "2059663872\n",
      "Epoch: 0, batch index: 71, learning rate: [0.01], loss:2.5774214267730713\n",
      "2059666944\n",
      "Epoch: 0, batch index: 72, learning rate: [0.01], loss:1.1604225635528564\n",
      "2059666944\n",
      "Epoch: 0, batch index: 73, learning rate: [0.01], loss:1.2660133838653564\n",
      "2059663360\n",
      "Epoch: 0, batch index: 74, learning rate: [0.01], loss:0.6979342103004456\n",
      "2059663360\n",
      "Epoch: 0, batch index: 75, learning rate: [0.01], loss:0.6674892902374268\n",
      "2059663360\n",
      "Epoch: 0, batch index: 76, learning rate: [0.01], loss:0.5877560377120972\n",
      "2059663360\n",
      "Epoch: 0, batch index: 77, learning rate: [0.01], loss:1.7060551643371582\n",
      "2059668992\n",
      "Epoch: 0, batch index: 78, learning rate: [0.01], loss:0.9128159880638123\n",
      "2059663872\n",
      "Epoch: 0, batch index: 79, learning rate: [0.01], loss:1.0668448209762573\n",
      "2059663360\n",
      "Epoch: 0, batch index: 80, learning rate: [0.01], loss:0.775256335735321\n",
      "2059663360\n",
      "Epoch: 0, batch index: 81, learning rate: [0.01], loss:1.5300076007843018\n",
      "2059663360\n",
      "Epoch: 0, batch index: 82, learning rate: [0.01], loss:1.429274559020996\n",
      "2059663360\n",
      "Epoch: 0, batch index: 83, learning rate: [0.01], loss:1.156768560409546\n",
      "2059663872\n",
      "Epoch: 0, batch index: 84, learning rate: [0.01], loss:0.6977500915527344\n",
      "2059663360\n",
      "Epoch: 0, batch index: 85, learning rate: [0.01], loss:1.368183970451355\n",
      "2059666432\n",
      "Epoch: 0, batch index: 86, learning rate: [0.01], loss:1.5929646492004395\n",
      "2059663360\n",
      "Epoch: 0, batch index: 87, learning rate: [0.01], loss:0.8007073402404785\n",
      "2059663872\n",
      "Epoch: 0, batch index: 88, learning rate: [0.01], loss:1.5549976825714111\n",
      "2059665408\n",
      "Epoch: 0, batch index: 89, learning rate: [0.01], loss:0.7896642088890076\n",
      "2059663360\n",
      "Epoch: 0, batch index: 90, learning rate: [0.01], loss:0.9351890683174133\n",
      "2059663872\n",
      "Epoch: 0, batch index: 91, learning rate: [0.01], loss:1.0372178554534912\n",
      "2059663360\n",
      "Epoch: 0, batch index: 92, learning rate: [0.01], loss:1.5610301494598389\n",
      "2059664896\n",
      "Epoch: 0, batch index: 93, learning rate: [0.01], loss:1.3262174129486084\n",
      "2059665408\n",
      "Epoch: 0, batch index: 94, learning rate: [0.01], loss:1.3506791591644287\n",
      "2059663360\n",
      "Epoch: 0, batch index: 95, learning rate: [0.001], loss:1.3734445571899414\n",
      "2059663872\n",
      "Epoch: 0, batch index: 96, learning rate: [0.001], loss:1.2726439237594604\n",
      "2059663872\n",
      "Epoch: 0, batch index: 97, learning rate: [0.001], loss:0.7489403486251831\n",
      "2059663360\n",
      "Epoch: 0, batch index: 98, learning rate: [0.001], loss:1.511220097541809\n",
      "2059665920\n",
      "Epoch: 0, batch index: 99, learning rate: [0.001], loss:1.2142010927200317\n",
      "2059663872\n",
      "Epoch: 0, batch index: 100, learning rate: [0.001], loss:1.0398122072219849\n",
      "2059663360\n",
      "Epoch: 0, batch index: 101, learning rate: [0.001], loss:1.0304226875305176\n",
      "2059663872\n",
      "Epoch: 0, batch index: 102, learning rate: [0.001], loss:1.038016438484192\n",
      "2059663872\n",
      "Epoch: 0, batch index: 103, learning rate: [0.001], loss:1.2393362522125244\n",
      "2059663872\n",
      "Epoch: 0, batch index: 104, learning rate: [0.001], loss:0.8021911382675171\n",
      "2059663872\n",
      "Epoch: 0, batch index: 105, learning rate: [0.001], loss:1.04152250289917\n",
      "2059663360\n",
      "Epoch: 0, batch index: 106, learning rate: [0.001], loss:1.282185673713684\n",
      "2059667456\n",
      "Epoch: 0, batch index: 107, learning rate: [0.001], loss:1.006746768951416\n",
      "2059679744\n",
      "Epoch: 0, batch index: 108, learning rate: [0.001], loss:0.9929687976837158\n",
      "2059663360\n",
      "Epoch: 0, batch index: 109, learning rate: [0.001], loss:1.2223551273345947\n",
      "2059665920\n",
      "Epoch: 0, batch index: 110, learning rate: [0.001], loss:1.0691460371017456\n",
      "2059663360\n",
      "Epoch: 0, batch index: 111, learning rate: [0.001], loss:1.1753792762756348\n",
      "2059663872\n",
      "Epoch: 0, batch index: 112, learning rate: [0.001], loss:0.8739793300628662\n",
      "2059663360\n",
      "Epoch: 0, batch index: 113, learning rate: [0.001], loss:1.3899558782577515\n",
      "2059664896\n",
      "Epoch: 0, batch index: 114, learning rate: [0.001], loss:1.183781623840332\n",
      "2059663360\n",
      "Epoch: 0, batch index: 115, learning rate: [0.001], loss:1.345781922340393\n",
      "2059665408\n",
      "Epoch: 0, batch index: 116, learning rate: [0.001], loss:1.5945041179656982\n",
      "2059667456\n",
      "Epoch: 0, batch index: 117, learning rate: [0.001], loss:1.205251932144165\n",
      "2059664384\n",
      "Epoch: 0, batch index: 118, learning rate: [0.001], loss:1.0867843627929688\n",
      "2059668480\n",
      "Epoch: 0, batch index: 119, learning rate: [0.001], loss:0.9496150612831116\n",
      "2059663872\n",
      "Epoch: 0, batch index: 120, learning rate: [0.001], loss:0.731055498123169\n",
      "2059663360\n",
      "Epoch: 0, batch index: 121, learning rate: [0.001], loss:1.0348353385925293\n",
      "2059664384\n",
      "Epoch: 0, batch index: 122, learning rate: [0.001], loss:0.5986510515213013\n",
      "2059663360\n",
      "Epoch: 0, batch index: 123, learning rate: [0.001], loss:1.1433855295181274\n",
      "2059663360\n",
      "Epoch: 0, batch index: 124, learning rate: [0.001], loss:1.0050969123840332\n",
      "2059663872\n",
      "Epoch: 0, batch index: 125, learning rate: [0.001], loss:0.9033002257347107\n",
      "2059663872\n",
      "Epoch: 0, batch index: 126, learning rate: [0.001], loss:0.9231845140457153\n",
      "2059663360\n",
      "Epoch: 0, batch index: 127, learning rate: [0.001], loss:1.0248523950576782\n",
      "2059664384\n",
      "Epoch: 0, batch index: 128, learning rate: [0.001], loss:0.8634496331214905\n",
      "2059663872\n",
      "Epoch: 0, batch index: 129, learning rate: [0.001], loss:0.6555377244949341\n",
      "2059663360\n",
      "Epoch: 0, batch index: 130, learning rate: [0.001], loss:0.6887535452842712\n",
      "2059663360\n",
      "Epoch: 0, batch index: 131, learning rate: [0.001], loss:0.7003543972969055\n",
      "2059663872\n",
      "Epoch: 0, batch index: 132, learning rate: [0.001], loss:0.8943033814430237\n",
      "2059663872\n",
      "Epoch: 0, batch index: 133, learning rate: [0.001], loss:1.2055842876434326\n",
      "2059665408\n",
      "Epoch: 0, batch index: 134, learning rate: [0.001], loss:0.7531620860099792\n",
      "2059663360\n",
      "Epoch: 0, batch index: 135, learning rate: [0.001], loss:1.3388687372207642\n",
      "2059663872\n",
      "Epoch: 0, batch index: 136, learning rate: [0.001], loss:1.0601997375488281\n",
      "2059663360\n",
      "Epoch: 0, batch index: 137, learning rate: [0.001], loss:1.1502808332443237\n",
      "2059665408\n",
      "Epoch: 0, batch index: 138, learning rate: [0.001], loss:1.231728196144104\n",
      "2059663360\n",
      "Epoch: 0, batch index: 139, learning rate: [0.001], loss:1.2410229444503784\n",
      "2059663872\n",
      "Epoch: 0, batch index: 140, learning rate: [0.001], loss:0.7877532243728638\n",
      "2059663872\n",
      "Epoch: 0, batch index: 141, learning rate: [0.001], loss:1.1518690586090088\n",
      "2059665408\n",
      "Epoch: 0, batch index: 142, learning rate: [0.001], loss:0.8782919049263\n",
      "2059663872\n",
      "Epoch: 0, batch index: 143, learning rate: [0.001], loss:1.05079984664917\n",
      "2059663360\n",
      "Epoch: 0, batch index: 144, learning rate: [0.001], loss:0.7915116548538208\n",
      "2059663360\n",
      "Epoch: 0, batch index: 145, learning rate: [0.001], loss:1.0805796384811401\n",
      "2059668480\n",
      "Epoch: 0, batch index: 146, learning rate: [0.001], loss:1.183458924293518\n",
      "2059665408\n",
      "Epoch: 0, batch index: 147, learning rate: [0.001], loss:0.8865253925323486\n",
      "2059663360\n",
      "Epoch: 0, batch index: 148, learning rate: [0.001], loss:0.6816542744636536\n",
      "2059663360\n",
      "Epoch: 0, batch index: 149, learning rate: [0.001], loss:1.3439801931381226\n",
      "2059664896\n",
      "Epoch: 0, batch index: 150, learning rate: [0.001], loss:1.3246686458587646\n",
      "2059663872\n",
      "Epoch: 0, batch index: 151, learning rate: [0.001], loss:1.1900266408920288\n",
      "2059663872\n",
      "Epoch: 0, batch index: 152, learning rate: [0.001], loss:1.3558436632156372\n",
      "2059663872\n",
      "Epoch: 0, batch index: 153, learning rate: [0.0001], loss:0.8978461623191833\n",
      "2059663360\n",
      "Epoch: 0, batch index: 154, learning rate: [0.0001], loss:1.2398407459259033\n",
      "2059663872\n",
      "Epoch: 0, batch index: 155, learning rate: [0.0001], loss:1.0333645343780518\n",
      "2059663360\n",
      "Epoch: 0, batch index: 156, learning rate: [0.0001], loss:0.8944311141967773\n",
      "2059663360\n",
      "Epoch: 0, batch index: 157, learning rate: [0.0001], loss:0.7145272493362427\n",
      "2059663360\n",
      "Epoch: 0, batch index: 158, learning rate: [0.0001], loss:1.3573369979858398\n",
      "2059663872\n",
      "Epoch: 0, batch index: 159, learning rate: [0.0001], loss:0.4990752935409546\n",
      "2059663360\n",
      "Epoch: 0, batch index: 160, learning rate: [0.0001], loss:1.2558457851409912\n",
      "2059663872\n",
      "Epoch: 0, batch index: 161, learning rate: [0.0001], loss:0.9079338312149048\n",
      "2059663360\n",
      "Epoch: 0, batch index: 162, learning rate: [0.0001], loss:0.9486101269721985\n",
      "2059663360\n",
      "Epoch: 0, batch index: 163, learning rate: [0.0001], loss:1.0287480354309082\n",
      "2059665920\n",
      "Epoch: 0, batch index: 164, learning rate: [0.0001], loss:0.912329912185669\n",
      "2059663360\n",
      "Epoch: 0, batch index: 165, learning rate: [0.0001], loss:0.7284549474716187\n",
      "2059663360\n",
      "Epoch: 0, batch index: 166, learning rate: [0.0001], loss:1.1949872970581055\n",
      "2059665408\n",
      "Epoch: 0, batch index: 167, learning rate: [0.0001], loss:1.0030033588409424\n",
      "2059663360\n",
      "Epoch: 0, batch index: 168, learning rate: [0.0001], loss:0.7539868354797363\n",
      "2059664896\n",
      "Epoch: 0, batch index: 169, learning rate: [0.0001], loss:0.7163236141204834\n",
      "2059663360\n",
      "Epoch: 0, batch index: 170, learning rate: [0.0001], loss:1.080228567123413\n",
      "2059663360\n",
      "Epoch: 0, batch index: 171, learning rate: [0.0001], loss:0.9298261404037476\n",
      "2059663360\n",
      "Epoch: 0, batch index: 172, learning rate: [0.0001], loss:1.2347028255462646\n",
      "2059663872\n",
      "Epoch: 0, batch index: 173, learning rate: [0.0001], loss:1.4087517261505127\n",
      "2059666432\n",
      "Epoch: 0, batch index: 174, learning rate: [0.0001], loss:0.8865518569946289\n",
      "2059663360\n",
      "Epoch: 0, batch index: 175, learning rate: [0.0001], loss:1.0962390899658203\n",
      "2059663360\n",
      "Epoch: 0, batch index: 176, learning rate: [0.0001], loss:0.8595911860466003\n",
      "2059663360\n",
      "Epoch: 0, batch index: 177, learning rate: [0.0001], loss:1.2097957134246826\n",
      "2059663872\n",
      "Epoch: 0, batch index: 178, learning rate: [0.0001], loss:0.910089373588562\n",
      "2059663872\n",
      "Epoch: 0, batch index: 179, learning rate: [0.0001], loss:0.7713321447372437\n",
      "2059663360\n",
      "Epoch: 0, batch index: 180, learning rate: [0.0001], loss:0.8759602904319763\n",
      "2059663360\n",
      "Epoch: 0, batch index: 181, learning rate: [0.0001], loss:1.2253473997116089\n",
      "2059674624\n",
      "Epoch: 0, batch index: 182, learning rate: [0.0001], loss:0.7249493598937988\n",
      "2059663360\n",
      "Epoch: 0, batch index: 183, learning rate: [0.0001], loss:0.7315512895584106\n",
      "2059663360\n",
      "Epoch: 0, batch index: 184, learning rate: [0.0001], loss:1.019858479499817\n",
      "2059663360\n",
      "Epoch: 0, batch index: 185, learning rate: [0.0001], loss:1.2640974521636963\n",
      "2059663872\n",
      "Epoch: 0, batch index: 186, learning rate: [0.0001], loss:0.4834955930709839\n",
      "2059663360\n",
      "Epoch: 0, batch index: 187, learning rate: [0.0001], loss:1.0591882467269897\n",
      "2059663360\n",
      "Epoch: 0, batch index: 188, learning rate: [0.0001], loss:1.3090465068817139\n",
      "2059672576\n",
      "Epoch: 0, batch index: 189, learning rate: [0.0001], loss:1.2046747207641602\n",
      "2059666944\n",
      "Epoch: 0, batch index: 190, learning rate: [0.0001], loss:0.8224846720695496\n",
      "2059663360\n",
      "Epoch: 0, batch index: 191, learning rate: [0.0001], loss:1.1774433851242065\n",
      "2059663360\n",
      "Epoch: 0, batch index: 192, learning rate: [0.0001], loss:1.182884693145752\n",
      "2059663360\n",
      "Epoch: 0, batch index: 193, learning rate: [0.0001], loss:0.57402503490448\n",
      "2059663360\n",
      "Epoch: 0, batch index: 194, learning rate: [0.0001], loss:1.3638145923614502\n",
      "2059663872\n",
      "Epoch: 0, batch index: 195, learning rate: [0.0001], loss:0.8678626418113708\n",
      "2059663360\n",
      "Epoch: 0, batch index: 196, learning rate: [0.0001], loss:0.8245209455490112\n",
      "2059664896\n",
      "Epoch: 0, batch index: 197, learning rate: [0.0001], loss:1.1282241344451904\n",
      "2059663872\n",
      "Epoch: 0, batch index: 198, learning rate: [0.0001], loss:0.6816427111625671\n",
      "2059663360\n",
      "Epoch: 0, batch index: 199, learning rate: [0.0001], loss:1.1043342351913452\n",
      "2059663360\n",
      "Epoch: 0, batch index: 200, learning rate: [0.0001], loss:0.9252533912658691\n",
      "2059664384\n",
      "Epoch: 0, batch index: 201, learning rate: [0.0001], loss:0.9887205362319946\n",
      "2059663360\n",
      "Epoch: 0, batch index: 202, learning rate: [0.0001], loss:0.9000437259674072\n",
      "2059663360\n",
      "Epoch: 0, batch index: 203, learning rate: [0.0001], loss:0.6931791305541992\n",
      "2059663360\n",
      "Epoch: 0, batch index: 204, learning rate: [0.0001], loss:0.9917551279067993\n",
      "2059664896\n",
      "Epoch: 0, batch index: 205, learning rate: [0.0001], loss:0.7840009927749634\n",
      "2059663872\n",
      "Epoch: 0, batch index: 206, learning rate: [0.0001], loss:1.2955985069274902\n",
      "2059679744\n",
      "Epoch: 0, batch index: 207, learning rate: [0.0001], loss:0.9571874737739563\n",
      "2059663872\n",
      "Epoch: 0, batch index: 208, learning rate: [0.0001], loss:1.0483431816101074\n",
      "2059665920\n",
      "Epoch: 0, batch index: 209, learning rate: [0.0001], loss:1.4662938117980957\n",
      "2059670016\n",
      "Epoch: 0, batch index: 210, learning rate: [0.0001], loss:0.9459387063980103\n",
      "2059663360\n",
      "Epoch: 0, batch index: 211, learning rate: [0.0001], loss:0.7566558122634888\n",
      "2059663360\n",
      "Epoch: 0, batch index: 212, learning rate: [0.0001], loss:0.909153938293457\n",
      "2059663872\n",
      "Epoch: 0, batch index: 213, learning rate: [0.0001], loss:0.9258134365081787\n",
      "2059663872\n",
      "Epoch: 0, batch index: 214, learning rate: [0.0001], loss:1.0814464092254639\n",
      "2059663872\n",
      "Epoch: 0, batch index: 215, learning rate: [0.0001], loss:0.7506308555603027\n",
      "2059663360\n",
      "Epoch: 0, batch index: 216, learning rate: [0.0001], loss:1.0947948694229126\n",
      "2059663360\n",
      "Epoch: 0, batch index: 217, learning rate: [0.0001], loss:0.8265987634658813\n",
      "2059663360\n",
      "Epoch: 0, batch index: 218, learning rate: [0.0001], loss:0.8899656534194946\n",
      "2059664896\n",
      "Epoch: 0, batch index: 219, learning rate: [0.0001], loss:0.524620532989502\n",
      "2059663360\n",
      "Epoch: 0, batch index: 220, learning rate: [0.0001], loss:1.0887826681137085\n",
      "2059663872\n",
      "Epoch: 0, batch index: 221, learning rate: [0.0001], loss:1.0113383531570435\n",
      "2059663872\n",
      "Epoch: 0, batch index: 222, learning rate: [0.0001], loss:0.7883737683296204\n",
      "2059663872\n",
      "Epoch: 0, batch index: 223, learning rate: [0.0001], loss:1.4047505855560303\n",
      "2059668480\n",
      "Epoch: 0, batch index: 224, learning rate: [0.0001], loss:1.161111831665039\n",
      "2059664384\n",
      "Epoch: 0, batch index: 225, learning rate: [0.0001], loss:0.9843072295188904\n",
      "2059663360\n",
      "Epoch: 0, batch index: 226, learning rate: [0.0001], loss:0.7770150899887085\n",
      "2059663360\n",
      "Epoch: 0, batch index: 227, learning rate: [0.0001], loss:0.8685203790664673\n",
      "2059663360\n",
      "Epoch: 0, batch index: 228, learning rate: [0.0001], loss:0.8380798101425171\n",
      "2059665408\n",
      "Epoch: 0, batch index: 229, learning rate: [0.0001], loss:0.9000293612480164\n",
      "2059665408\n",
      "Epoch: 0, batch index: 230, learning rate: [0.0001], loss:1.11795973777771\n",
      "2059671552\n",
      "Epoch: 0, batch index: 231, learning rate: [0.0001], loss:1.2040822505950928\n",
      "2059666432\n",
      "Epoch: 0, batch index: 232, learning rate: [0.0001], loss:0.9421709179878235\n",
      "2059663360\n",
      "Epoch: 0, batch index: 233, learning rate: [0.0001], loss:1.5404534339904785\n",
      "2059672576\n",
      "Epoch: 0, batch index: 234, learning rate: [0.0001], loss:0.704252302646637\n",
      "2059663360\n",
      "Epoch: 0, batch index: 235, learning rate: [0.0001], loss:0.6692916750907898\n",
      "2059664384\n",
      "Epoch: 0, batch index: 236, learning rate: [0.0001], loss:0.5366280674934387\n",
      "2059663360\n",
      "Epoch: 0, batch index: 237, learning rate: [0.0001], loss:1.102932095527649\n",
      "2059663360\n",
      "Epoch: 0, batch index: 238, learning rate: [1e-05], loss:0.9259170293807983\n",
      "2059663360\n",
      "Epoch: 0, batch index: 239, learning rate: [1e-05], loss:0.4209958016872406\n",
      "2059663360\n",
      "Epoch: 0, batch index: 240, learning rate: [1e-05], loss:1.2754268646240234\n",
      "2059664384\n",
      "Epoch: 0, batch index: 241, learning rate: [1e-05], loss:0.6461315751075745\n",
      "2059663360\n",
      "Epoch: 0, batch index: 242, learning rate: [1e-05], loss:0.9873002171516418\n",
      "2059663872\n",
      "Epoch: 0, batch index: 243, learning rate: [1e-05], loss:0.7313071489334106\n",
      "2059663360\n",
      "Epoch: 0, batch index: 244, learning rate: [1e-05], loss:0.9873403310775757\n",
      "2059663872\n",
      "Epoch: 0, batch index: 245, learning rate: [1e-05], loss:0.6296961903572083\n",
      "2059663360\n",
      "Epoch: 0, batch index: 246, learning rate: [1e-05], loss:1.0890040397644043\n",
      "2059672064\n",
      "Epoch: 0, batch index: 247, learning rate: [1e-05], loss:0.8707045316696167\n",
      "2059663872\n",
      "Epoch: 0, batch index: 248, learning rate: [1e-05], loss:0.759310781955719\n",
      "2059667456\n",
      "Epoch: 0, batch index: 249, learning rate: [1e-05], loss:1.0018342733383179\n",
      "2059663872\n",
      "Epoch: 0, batch index: 250, learning rate: [1e-05], loss:0.935489296913147\n",
      "2059663360\n",
      "Epoch: 0, batch index: 251, learning rate: [1e-05], loss:1.1832787990570068\n",
      "2059663872\n",
      "Epoch: 0, batch index: 252, learning rate: [1e-05], loss:1.1293177604675293\n",
      "2059663360\n",
      "Epoch: 0, batch index: 253, learning rate: [1e-05], loss:0.9628329873085022\n",
      "2059666944\n",
      "Epoch: 0, batch index: 254, learning rate: [1e-05], loss:0.9600027799606323\n",
      "2059663872\n",
      "Epoch: 0, batch index: 255, learning rate: [1e-05], loss:1.3117916584014893\n",
      "2059663872\n",
      "Epoch: 0, batch index: 256, learning rate: [1e-05], loss:0.9238224625587463\n",
      "2059681280\n",
      "Epoch: 0, batch index: 257, learning rate: [1e-05], loss:0.8066282868385315\n",
      "2059668992\n",
      "Epoch: 0, batch index: 258, learning rate: [1e-05], loss:0.639310359954834\n",
      "2059663872\n",
      "Epoch: 0, batch index: 259, learning rate: [1e-05], loss:0.9089215397834778\n",
      "2059663360\n",
      "Epoch: 0, batch index: 260, learning rate: [1e-05], loss:1.1151256561279297\n",
      "2059663360\n",
      "Epoch: 0, batch index: 261, learning rate: [1e-05], loss:0.975031852722168\n",
      "2059665408\n",
      "Epoch: 0, batch index: 262, learning rate: [1e-05], loss:0.9861295223236084\n",
      "2059668480\n",
      "Epoch: 0, batch index: 263, learning rate: [1e-05], loss:1.2571935653686523\n",
      "2059667968\n",
      "Epoch: 0, batch index: 264, learning rate: [1e-05], loss:0.6579354405403137\n",
      "2059663872\n",
      "Epoch: 0, batch index: 265, learning rate: [1e-05], loss:0.6214380264282227\n",
      "2059663360\n",
      "Epoch: 0, batch index: 266, learning rate: [1e-05], loss:0.9540809988975525\n",
      "2059669504\n",
      "Epoch: 0, batch index: 267, learning rate: [1e-05], loss:1.0501946210861206\n",
      "2059663360\n",
      "Epoch: 0, batch index: 268, learning rate: [1e-05], loss:1.3863400220870972\n",
      "2059668992\n",
      "Epoch: 0, batch index: 269, learning rate: [1e-05], loss:0.9279956817626953\n",
      "2059664384\n",
      "Epoch: 0, batch index: 270, learning rate: [1e-05], loss:0.7021036148071289\n",
      "2059663360\n",
      "Epoch: 0, batch index: 271, learning rate: [1e-05], loss:0.9373342394828796\n",
      "2059663872\n",
      "Epoch: 0, batch index: 272, learning rate: [1e-05], loss:0.6794241070747375\n",
      "2059663360\n",
      "Epoch: 0, batch index: 273, learning rate: [1e-05], loss:0.5285091400146484\n",
      "2059663360\n",
      "Epoch: 0, batch index: 274, learning rate: [1e-05], loss:0.5708179473876953\n",
      "2059663360\n",
      "Epoch: 0, batch index: 275, learning rate: [1e-05], loss:0.8517950177192688\n",
      "2059663360\n",
      "Epoch: 0, batch index: 276, learning rate: [1e-05], loss:0.7497098445892334\n",
      "2059663360\n",
      "Epoch: 0, batch index: 277, learning rate: [1e-05], loss:1.3299599885940552\n",
      "2059663872\n",
      "Epoch: 0, batch index: 278, learning rate: [1e-05], loss:0.5833398699760437\n",
      "2059663360\n",
      "Epoch: 0, batch index: 279, learning rate: [1e-05], loss:1.0957098007202148\n",
      "2059664896\n",
      "Epoch: 0, batch index: 280, learning rate: [1e-05], loss:1.4124101400375366\n",
      "2059666432\n",
      "Epoch: 0, batch index: 281, learning rate: [1e-05], loss:1.0842283964157104\n",
      "2059664384\n",
      "Epoch: 0, batch index: 282, learning rate: [1e-05], loss:0.5119301080703735\n",
      "2059663360\n",
      "Epoch: 0, batch index: 283, learning rate: [1e-05], loss:1.2113547325134277\n",
      "2059667456\n",
      "Epoch: 0, batch index: 284, learning rate: [1e-05], loss:1.01534104347229\n",
      "2059663872\n",
      "Epoch: 0, batch index: 285, learning rate: [1e-05], loss:0.6072757244110107\n",
      "2059663360\n",
      "Epoch: 0, batch index: 286, learning rate: [1e-05], loss:0.9805850982666016\n",
      "2059664384\n",
      "Epoch: 0, batch index: 287, learning rate: [1e-05], loss:1.1690726280212402\n",
      "2059665920\n",
      "Epoch: 0, batch index: 288, learning rate: [1e-05], loss:0.4870140254497528\n",
      "2059663360\n",
      "Epoch: 0, batch index: 289, learning rate: [1e-05], loss:0.9415755867958069\n",
      "2059663872\n",
      "Epoch: 0, batch index: 290, learning rate: [1e-05], loss:0.5670959949493408\n",
      "2059663360\n",
      "Epoch: 0, batch index: 291, learning rate: [1e-05], loss:1.4036047458648682\n",
      "2059666944\n",
      "Epoch: 0, batch index: 292, learning rate: [1e-05], loss:0.9084668159484863\n",
      "2059663872\n",
      "Epoch: 0, batch index: 293, learning rate: [1e-05], loss:0.596716046333313\n",
      "2059663360\n",
      "Epoch: 0, batch index: 294, learning rate: [1e-05], loss:0.5775853395462036\n",
      "2059663360\n",
      "Epoch: 0, batch index: 295, learning rate: [1e-05], loss:1.0989331007003784\n",
      "2059664896\n",
      "Epoch: 0, batch index: 296, learning rate: [1.0000000000000002e-06], loss:1.0803463459014893\n",
      "2059663360\n",
      "Epoch: 0, batch index: 297, learning rate: [1.0000000000000002e-06], loss:0.8868269920349121\n",
      "2059664896\n",
      "Epoch: 0, batch index: 298, learning rate: [1.0000000000000002e-06], loss:0.3082117438316345\n",
      "2059663360\n",
      "Epoch: 0, batch index: 299, learning rate: [1.0000000000000002e-06], loss:1.1321555376052856\n",
      "2059663360\n",
      "Epoch: 0, batch index: 300, learning rate: [1.0000000000000002e-06], loss:1.0756911039352417\n",
      "2059665408\n",
      "Epoch: 0, batch index: 301, learning rate: [1.0000000000000002e-06], loss:0.7978410720825195\n",
      "2059663872\n",
      "Epoch: 0, batch index: 302, learning rate: [1.0000000000000002e-06], loss:0.7572543621063232\n",
      "2059663872\n",
      "Epoch: 0, batch index: 303, learning rate: [1.0000000000000002e-06], loss:0.7612196207046509\n",
      "2059663360\n",
      "Epoch: 0, batch index: 304, learning rate: [1.0000000000000002e-06], loss:1.4766852855682373\n",
      "2059664384\n",
      "Epoch: 0, batch index: 305, learning rate: [1.0000000000000002e-06], loss:1.0811352729797363\n",
      "2059663360\n",
      "Epoch: 0, batch index: 306, learning rate: [1.0000000000000002e-06], loss:0.8720085620880127\n",
      "2059665408\n",
      "Epoch: 0, batch index: 307, learning rate: [1.0000000000000002e-06], loss:0.9225767850875854\n",
      "2059663360\n",
      "Epoch: 0, batch index: 308, learning rate: [1.0000000000000002e-06], loss:1.0230144262313843\n",
      "2059663360\n",
      "Epoch: 0, batch index: 309, learning rate: [1.0000000000000002e-06], loss:0.9505442380905151\n",
      "2059663872\n",
      "Epoch: 0, batch index: 310, learning rate: [1.0000000000000002e-06], loss:0.9123831391334534\n",
      "2059663360\n",
      "Epoch: 0, batch index: 311, learning rate: [1.0000000000000002e-06], loss:0.766730785369873\n",
      "2059663360\n",
      "Epoch: 0, batch index: 312, learning rate: [1.0000000000000002e-06], loss:0.9931095838546753\n",
      "2059663360\n",
      "Epoch: 0, batch index: 313, learning rate: [1.0000000000000002e-06], loss:0.5642114281654358\n",
      "2059663360\n",
      "Epoch: 0, batch index: 314, learning rate: [1.0000000000000002e-06], loss:0.6394611597061157\n",
      "2059663360\n",
      "Epoch: 0, batch index: 315, learning rate: [1.0000000000000002e-06], loss:1.1345232725143433\n",
      "2059664896\n",
      "Epoch: 0, batch index: 316, learning rate: [1.0000000000000002e-06], loss:1.3955832719802856\n",
      "2059664896\n",
      "Epoch: 0, batch index: 317, learning rate: [1.0000000000000002e-06], loss:0.5226015448570251\n",
      "2059663360\n",
      "Epoch: 0, batch index: 318, learning rate: [1.0000000000000002e-06], loss:0.8888401985168457\n",
      "2059664896\n",
      "Epoch: 0, batch index: 319, learning rate: [1.0000000000000002e-06], loss:0.7282558083534241\n",
      "2059663360\n",
      "Epoch: 0, batch index: 320, learning rate: [1.0000000000000002e-06], loss:0.8383535146713257\n",
      "2059663872\n",
      "Epoch: 0, batch index: 321, learning rate: [1.0000000000000002e-06], loss:1.2215006351470947\n",
      "2059665408\n",
      "Epoch: 0, batch index: 322, learning rate: [1.0000000000000002e-06], loss:1.2780063152313232\n",
      "2059667968\n",
      "Epoch: 0, batch index: 323, learning rate: [1.0000000000000002e-06], loss:1.0747747421264648\n",
      "2059665920\n",
      "Epoch: 0, batch index: 324, learning rate: [1.0000000000000002e-06], loss:1.448419213294983\n",
      "2059667968\n",
      "Epoch: 0, batch index: 325, learning rate: [1.0000000000000002e-06], loss:0.7353627681732178\n",
      "2059663360\n",
      "Epoch: 0, batch index: 326, learning rate: [1.0000000000000002e-06], loss:1.0781900882720947\n",
      "2059665920\n",
      "Epoch: 0, batch index: 327, learning rate: [1.0000000000000002e-06], loss:0.8856850266456604\n",
      "2059663360\n",
      "Epoch: 0, batch index: 328, learning rate: [1.0000000000000002e-06], loss:1.0924252271652222\n",
      "2059663360\n",
      "Epoch: 0, batch index: 329, learning rate: [1.0000000000000002e-06], loss:0.8651626706123352\n",
      "2059663872\n",
      "Epoch: 0, batch index: 330, learning rate: [1.0000000000000002e-06], loss:0.7887470722198486\n",
      "2059663872\n",
      "Epoch: 0, batch index: 331, learning rate: [1.0000000000000002e-06], loss:0.535990834236145\n",
      "2059663360\n",
      "Epoch: 0, batch index: 332, learning rate: [1.0000000000000002e-06], loss:1.0635837316513062\n",
      "2059664896\n",
      "Epoch: 0, batch index: 333, learning rate: [1.0000000000000002e-06], loss:0.750974178314209\n",
      "2059664896\n",
      "Epoch: 0, batch index: 334, learning rate: [1.0000000000000002e-06], loss:0.7577263116836548\n",
      "2059663360\n",
      "Epoch: 0, batch index: 335, learning rate: [1.0000000000000002e-06], loss:0.8388921022415161\n",
      "2059663872\n",
      "Epoch: 0, batch index: 336, learning rate: [1.0000000000000002e-06], loss:0.9163206815719604\n",
      "2059663872\n",
      "Epoch: 0, batch index: 337, learning rate: [1.0000000000000002e-06], loss:0.4466525912284851\n",
      "2059663360\n",
      "Epoch: 0, batch index: 338, learning rate: [1.0000000000000002e-06], loss:0.9142701029777527\n",
      "2059665408\n",
      "Epoch: 0, batch index: 339, learning rate: [1.0000000000000002e-06], loss:0.5025275349617004\n",
      "2059663360\n",
      "Epoch: 0, batch index: 340, learning rate: [1.0000000000000002e-06], loss:1.04381263256073\n",
      "2059664896\n",
      "Epoch: 0, batch index: 341, learning rate: [1.0000000000000002e-06], loss:1.165878415107727\n",
      "2059663872\n",
      "Epoch: 0, batch index: 342, learning rate: [1.0000000000000002e-06], loss:1.1306734085083008\n",
      "2059663360\n",
      "Epoch: 0, batch index: 343, learning rate: [1.0000000000000002e-06], loss:0.8826443552970886\n",
      "2059663360\n",
      "Epoch: 0, batch index: 344, learning rate: [1.0000000000000002e-06], loss:0.5367476344108582\n",
      "2059663360\n",
      "Epoch: 0, batch index: 345, learning rate: [1.0000000000000002e-06], loss:0.8974490165710449\n",
      "2059663872\n",
      "Epoch: 0, batch index: 346, learning rate: [1.0000000000000002e-06], loss:1.1445612907409668\n",
      "2059663360\n",
      "Epoch: 0, batch index: 347, learning rate: [1.0000000000000002e-06], loss:1.0512173175811768\n",
      "2059663872\n",
      "Epoch: 0, batch index: 348, learning rate: [1.0000000000000002e-06], loss:0.5616833567619324\n",
      "2059663360\n",
      "Epoch: 0, batch index: 349, learning rate: [1.0000000000000002e-06], loss:1.2916624546051025\n",
      "2059675648\n",
      "Epoch: 0, batch index: 350, learning rate: [1.0000000000000002e-06], loss:0.8634979724884033\n",
      "2059663360\n",
      "Epoch: 0, batch index: 351, learning rate: [1.0000000000000002e-06], loss:1.0277178287506104\n",
      "2059663360\n",
      "Epoch: 0, batch index: 352, learning rate: [1.0000000000000002e-06], loss:0.8374176025390625\n",
      "2059663360\n",
      "Epoch: 0, batch index: 353, learning rate: [1.0000000000000002e-06], loss:0.9121867418289185\n",
      "2059663360\n",
      "Epoch: 0, batch index: 354, learning rate: [1.0000000000000002e-07], loss:0.9200403690338135\n",
      "2059663360\n",
      "Epoch: 0, batch index: 355, learning rate: [1.0000000000000002e-07], loss:0.7475004196166992\n",
      "2059663872\n",
      "Epoch: 0, batch index: 356, learning rate: [1.0000000000000002e-07], loss:0.8624074459075928\n",
      "2059663360\n",
      "Epoch: 0, batch index: 357, learning rate: [1.0000000000000002e-07], loss:1.2008159160614014\n",
      "2059668992\n",
      "Epoch: 0, batch index: 358, learning rate: [1.0000000000000002e-07], loss:0.5598945617675781\n",
      "2059663360\n",
      "Epoch: 0, batch index: 359, learning rate: [1.0000000000000002e-07], loss:0.7457095384597778\n",
      "2059663360\n",
      "Epoch: 0, batch index: 360, learning rate: [1.0000000000000002e-07], loss:0.9767928719520569\n",
      "2059663360\n",
      "Epoch: 0, batch index: 361, learning rate: [1.0000000000000002e-07], loss:1.076411485671997\n",
      "2059663872\n",
      "Epoch: 0, batch index: 362, learning rate: [1.0000000000000002e-07], loss:1.1241610050201416\n",
      "2059665408\n",
      "Epoch: 0, batch index: 363, learning rate: [1.0000000000000002e-07], loss:0.9180050492286682\n",
      "2059663360\n",
      "Epoch: 0, batch index: 364, learning rate: [1.0000000000000002e-07], loss:0.8281959295272827\n",
      "2059663360\n",
      "Epoch: 0, batch index: 365, learning rate: [1.0000000000000002e-07], loss:0.6396939754486084\n",
      "2059663360\n",
      "Epoch: 0, batch index: 366, learning rate: [1.0000000000000002e-07], loss:0.9584512114524841\n",
      "2059663360\n",
      "Epoch: 0, batch index: 367, learning rate: [1.0000000000000002e-07], loss:1.235831379890442\n",
      "2059663872\n",
      "Epoch: 0, batch index: 368, learning rate: [1.0000000000000002e-07], loss:0.7551101446151733\n",
      "2059663360\n",
      "Epoch: 0, batch index: 369, learning rate: [1.0000000000000002e-07], loss:0.9211327433586121\n",
      "2059663360\n",
      "Epoch: 0, batch index: 370, learning rate: [1.0000000000000002e-07], loss:1.3703978061676025\n",
      "2059678720\n",
      "Epoch: 0, batch index: 371, learning rate: [1.0000000000000002e-07], loss:1.2847403287887573\n",
      "2059664896\n",
      "Epoch: 0, batch index: 372, learning rate: [1.0000000000000002e-07], loss:1.3866052627563477\n",
      "2059668480\n",
      "Epoch: 0, batch index: 373, learning rate: [1.0000000000000002e-07], loss:1.119804859161377\n",
      "2059663872\n",
      "Epoch: 0, batch index: 374, learning rate: [1.0000000000000002e-07], loss:0.7220210433006287\n",
      "2059663360\n",
      "Epoch: 0, batch index: 375, learning rate: [1.0000000000000002e-07], loss:1.3562133312225342\n",
      "2059663872\n",
      "Epoch: 0, batch index: 376, learning rate: [1.0000000000000002e-07], loss:0.6535887718200684\n",
      "2059663360\n",
      "Epoch: 0, batch index: 377, learning rate: [1.0000000000000002e-07], loss:0.7216932773590088\n",
      "2059663360\n",
      "Epoch: 0, batch index: 378, learning rate: [1.0000000000000002e-07], loss:1.036395788192749\n",
      "2059665920\n",
      "Epoch: 0, batch index: 379, learning rate: [1.0000000000000002e-07], loss:0.8976452350616455\n",
      "2059663872\n",
      "Epoch: 0, batch index: 380, learning rate: [1.0000000000000002e-07], loss:1.040910005569458\n",
      "2059665920\n",
      "Epoch: 0, batch index: 381, learning rate: [1.0000000000000002e-07], loss:1.2610187530517578\n",
      "2059663360\n",
      "Epoch: 0, batch index: 382, learning rate: [1.0000000000000002e-07], loss:0.8400036096572876\n",
      "2059663360\n",
      "Epoch: 0, batch index: 383, learning rate: [1.0000000000000002e-07], loss:1.1135975122451782\n",
      "2059663360\n",
      "Epoch: 0, batch index: 384, learning rate: [1.0000000000000002e-07], loss:0.6838151812553406\n",
      "2059663360\n",
      "Epoch: 0, batch index: 385, learning rate: [1.0000000000000002e-07], loss:0.7619277238845825\n",
      "2059663360\n",
      "Epoch: 0, batch index: 386, learning rate: [1.0000000000000002e-07], loss:0.8546997308731079\n",
      "2059663872\n",
      "Epoch: 0, batch index: 387, learning rate: [1.0000000000000002e-07], loss:0.9824298620223999\n",
      "2059663360\n",
      "Epoch: 0, batch index: 388, learning rate: [1.0000000000000002e-07], loss:1.491282343864441\n",
      "2059666944\n",
      "Epoch: 0, batch index: 389, learning rate: [1.0000000000000002e-07], loss:1.1734938621520996\n",
      "2059668992\n",
      "Epoch: 0, batch index: 390, learning rate: [1.0000000000000002e-07], loss:0.8555382490158081\n",
      "2059663360\n",
      "Epoch: 0, batch index: 391, learning rate: [1.0000000000000002e-07], loss:0.8681572675704956\n",
      "2059663360\n",
      "Epoch: 0, batch index: 392, learning rate: [1.0000000000000002e-07], loss:0.6563192009925842\n",
      "2059663872\n",
      "Epoch: 0, batch index: 393, learning rate: [1.0000000000000002e-07], loss:1.0786583423614502\n",
      "2059663360\n",
      "Epoch: 0, batch index: 394, learning rate: [1.0000000000000002e-07], loss:1.1992838382720947\n",
      "2059663360\n",
      "Epoch: 0, batch index: 395, learning rate: [1.0000000000000002e-07], loss:1.298309564590454\n",
      "2059663872\n",
      "Epoch: 0, batch index: 396, learning rate: [1.0000000000000002e-07], loss:1.008878469467163\n",
      "2059668992\n",
      "Epoch: 0, batch index: 397, learning rate: [1.0000000000000002e-07], loss:1.1613457202911377\n",
      "2059663872\n",
      "Epoch: 0, batch index: 398, learning rate: [1.0000000000000002e-07], loss:1.004135012626648\n",
      "2059663872\n",
      "Epoch: 0, batch index: 399, learning rate: [1.0000000000000002e-07], loss:0.6378822326660156\n",
      "2059663360\n",
      "Epoch: 0, batch index: 400, learning rate: [1.0000000000000002e-07], loss:1.0303747653961182\n",
      "2059664896\n",
      "Epoch: 0, batch index: 401, learning rate: [1.0000000000000002e-07], loss:1.0223227739334106\n",
      "2059663872\n",
      "Epoch: 0, batch index: 402, learning rate: [1.0000000000000002e-07], loss:1.1687180995941162\n",
      "2059664384\n",
      "Epoch: 0, batch index: 403, learning rate: [1.0000000000000002e-07], loss:0.8071444034576416\n",
      "2059663360\n",
      "Epoch: 0, batch index: 404, learning rate: [1.0000000000000002e-07], loss:0.9451923370361328\n",
      "2059663360\n",
      "Epoch: 0, batch index: 405, learning rate: [1.0000000000000002e-07], loss:0.6716534495353699\n",
      "2059663360\n",
      "Epoch: 0, batch index: 406, learning rate: [1.0000000000000002e-07], loss:0.6938619613647461\n",
      "2059663360\n",
      "Epoch: 0, batch index: 407, learning rate: [1.0000000000000002e-07], loss:0.7020500898361206\n",
      "2059663360\n",
      "Epoch: 0, batch index: 408, learning rate: [1.0000000000000002e-07], loss:1.0201966762542725\n",
      "2059663360\n",
      "Epoch: 0, batch index: 409, learning rate: [1.0000000000000002e-07], loss:1.220445990562439\n",
      "2059664384\n",
      "Epoch: 0, batch index: 410, learning rate: [1.0000000000000002e-07], loss:1.0636330842971802\n",
      "2059663360\n",
      "Epoch: 0, batch index: 411, learning rate: [1.0000000000000002e-07], loss:1.0266742706298828\n",
      "2059663360\n",
      "Epoch: 0, batch index: 412, learning rate: [1.0000000000000004e-08], loss:0.974140465259552\n",
      "2059663872\n",
      "Epoch: 0, batch index: 413, learning rate: [1.0000000000000004e-08], loss:1.1221435070037842\n",
      "2059663360\n",
      "Epoch: 0, batch index: 414, learning rate: [1.0000000000000004e-08], loss:1.0690088272094727\n",
      "2059664384\n",
      "Epoch: 0, batch index: 415, learning rate: [1.0000000000000004e-08], loss:0.8341885805130005\n",
      "2059663872\n",
      "Epoch: 0, batch index: 416, learning rate: [1.0000000000000004e-08], loss:0.35754305124282837\n",
      "2059663360\n",
      "Epoch: 0, batch index: 417, learning rate: [1.0000000000000004e-08], loss:0.6776642203330994\n",
      "2059663360\n",
      "Epoch: 0, batch index: 418, learning rate: [1.0000000000000004e-08], loss:1.033097267150879\n",
      "2059667456\n",
      "Epoch: 0, batch index: 419, learning rate: [1.0000000000000004e-08], loss:1.2544569969177246\n",
      "2059664896\n",
      "Epoch: 0, batch index: 420, learning rate: [1.0000000000000004e-08], loss:1.2238106727600098\n",
      "2059665408\n",
      "Epoch: 0, batch index: 421, learning rate: [1.0000000000000004e-08], loss:1.128159999847412\n",
      "2059667456\n",
      "Epoch: 0, batch index: 422, learning rate: [1.0000000000000004e-08], loss:1.0204187631607056\n",
      "2059663872\n",
      "Epoch: 0, batch index: 423, learning rate: [1.0000000000000004e-08], loss:1.6200025081634521\n",
      "2059670528\n",
      "Epoch: 0, batch index: 424, learning rate: [1.0000000000000004e-08], loss:0.6987814903259277\n",
      "2059663360\n",
      "Epoch: 0, batch index: 425, learning rate: [1.0000000000000004e-08], loss:1.0339915752410889\n",
      "2059663872\n",
      "Epoch: 0, batch index: 426, learning rate: [1.0000000000000004e-08], loss:1.0958210229873657\n",
      "2059663872\n",
      "Epoch: 0, batch index: 427, learning rate: [1.0000000000000004e-08], loss:1.6696122884750366\n",
      "2059670016\n",
      "Epoch: 0, batch index: 428, learning rate: [1.0000000000000004e-08], loss:1.2371217012405396\n",
      "2059663360\n",
      "Epoch: 0, batch index: 429, learning rate: [1.0000000000000004e-08], loss:1.0591707229614258\n",
      "2059674112\n",
      "Epoch: 0, batch index: 430, learning rate: [1.0000000000000004e-08], loss:0.9381706118583679\n",
      "2059663360\n",
      "Epoch: 0, batch index: 431, learning rate: [1.0000000000000004e-08], loss:0.8640357255935669\n",
      "2059668480\n",
      "Epoch: 0, batch index: 432, learning rate: [1.0000000000000004e-08], loss:1.1911215782165527\n",
      "2059663872\n",
      "Epoch: 0, batch index: 433, learning rate: [1.0000000000000004e-08], loss:1.429589033126831\n",
      "2059673600\n",
      "Epoch: 0, batch index: 434, learning rate: [1.0000000000000004e-08], loss:0.8944098353385925\n",
      "2059663360\n",
      "Epoch: 0, batch index: 435, learning rate: [1.0000000000000004e-08], loss:0.6464036107063293\n",
      "2059664896\n",
      "Epoch: 0, batch index: 436, learning rate: [1.0000000000000004e-08], loss:0.5204154849052429\n",
      "2059663360\n",
      "Epoch: 0, batch index: 437, learning rate: [1.0000000000000004e-08], loss:0.4010277986526489\n",
      "2059663360\n",
      "Epoch: 0, batch index: 438, learning rate: [1.0000000000000004e-08], loss:0.8743283748626709\n",
      "2059663360\n",
      "Epoch: 0, batch index: 439, learning rate: [1.0000000000000004e-08], loss:1.1196495294570923\n",
      "2059667456\n",
      "Epoch: 0, batch index: 440, learning rate: [1.0000000000000004e-08], loss:0.7880209684371948\n",
      "2059663360\n",
      "Epoch: 0, batch index: 441, learning rate: [1.0000000000000004e-08], loss:1.2455799579620361\n",
      "2059667456\n",
      "Epoch: 0, batch index: 442, learning rate: [1.0000000000000004e-08], loss:0.6278538703918457\n",
      "2059663872\n",
      "Epoch: 0, batch index: 443, learning rate: [1.0000000000000004e-08], loss:0.441252738237381\n",
      "2059663360\n",
      "Epoch: 0, batch index: 444, learning rate: [1.0000000000000004e-08], loss:1.5215039253234863\n",
      "2059671552\n",
      "Epoch: 0, batch index: 445, learning rate: [1.0000000000000004e-08], loss:1.2466024160385132\n",
      "2059666944\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.30 GiB. GPU 0 has a total capacity of 9.77 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 8.02 GiB memory in use. Of the allocated memory 7.65 GiB is allocated by PyTorch, and 93.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, validation_loader, epochs, learning_rate, model_name)\u001b[0m\n\u001b[1;32m     35\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m features)\n\u001b[1;32m     36\u001b[0m labels \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m labels]\n\u001b[0;32m---> 38\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Calculate the loss via cross_entropy\u001b[39;00m\n\u001b[1;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:379\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets should not be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 379\u001b[0m labels, matched_gt_boxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_targets_to_anchors\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m regression_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mencode(matched_gt_boxes, anchors)\n\u001b[1;32m    381\u001b[0m loss_objectness, loss_rpn_box_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(\n\u001b[1;32m    382\u001b[0m     objectness, pred_bbox_deltas, labels, regression_targets\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torchvision/models/detection/rpn.py:208\u001b[0m, in \u001b[0;36mRegionProposalNetwork.assign_targets_to_anchors\u001b[0;34m(self, anchors, targets)\u001b[0m\n\u001b[1;32m    206\u001b[0m     labels_per_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((anchors_per_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     match_quality_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manchors_per_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproposal_matcher(match_quality_matrix)\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;66;03m# get the targets corresponding GT for each proposal\u001b[39;00m\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;66;03m# NB: need to clamp the indices because we can have a single\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;66;03m# GT in the image, and matched_idxs can be -2, which goes\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# out of bounds\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torchvision/ops/boxes.py:287\u001b[0m, in \u001b[0;36mbox_iou\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    286\u001b[0m     _log_api_usage_once(box_iou)\n\u001b[0;32m--> 287\u001b[0m inter, union \u001b[38;5;241m=\u001b[39m \u001b[43m_box_inter_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    288\u001b[0m iou \u001b[38;5;241m=\u001b[39m inter \u001b[38;5;241m/\u001b[39m union\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iou\n",
      "File \u001b[0;32m~/anaconda3/envs/Deep_Learning/lib/python3.10/site-packages/torchvision/ops/boxes.py:263\u001b[0m, in \u001b[0;36m_box_inter_union\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    260\u001b[0m lt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, :\u001b[38;5;241m2\u001b[39m], boxes2[:, :\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[1;32m    261\u001b[0m rb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(boxes1[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m2\u001b[39m:], boxes2[:, \u001b[38;5;241m2\u001b[39m:])  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m wh \u001b[38;5;241m=\u001b[39m \u001b[43m_upcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N,M,2]\u001b[39;00m\n\u001b[1;32m    264\u001b[0m inter \u001b[38;5;241m=\u001b[39m wh[:, :, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m wh[:, :, \u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# [N,M]\u001b[39;00m\n\u001b[1;32m    266\u001b[0m union \u001b[38;5;241m=\u001b[39m area1[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m area2 \u001b[38;5;241m-\u001b[39m inter\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.30 GiB. GPU 0 has a total capacity of 9.77 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 8.02 GiB memory in use. Of the allocated memory 7.65 GiB is allocated by PyTorch, and 93.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "train(model, epochs=1, learning_rate=0.01, train_loader=train_loader, validation_loader=valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'weights/best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2697, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_from_valiadation(model, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "** (xviewer:30562): CRITICAL **: 13:49:08.254: xviewer_list_store_get_pos_by_image: assertion 'XVIEWER_IS_IMAGE (image)' failed\n",
      "\n",
      "** (xviewer:30562): CRITICAL **: 13:49:08.256: xviewer_image_get_file: assertion 'XVIEWER_IS_IMAGE (img)' failed\n",
      "\n",
      "(xviewer:30562): GLib-GIO-CRITICAL **: 13:49:08.256: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n",
      "\n",
      "** (xviewer:30562): CRITICAL **: 13:49:08.479: xviewer_list_store_get_pos_by_image: assertion 'XVIEWER_IS_IMAGE (image)' failed\n",
      "\n",
      "** (xviewer:30562): CRITICAL **: 13:49:08.479: xviewer_image_get_file: assertion 'XVIEWER_IS_IMAGE (img)' failed\n",
      "\n",
      "(xviewer:30562): GLib-GIO-CRITICAL **: 13:49:08.479: g_file_equal: assertion 'G_IS_FILE (file1)' failed\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "batch = next(iter(valid_loader))\n",
    "\n",
    "features, labels = batch\n",
    "\n",
    "predictions = model([img.to(device) for img in features])\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "for img, box, label in zip(features, predictions, labels):\n",
    "    with torch.no_grad():\n",
    "        formatted_boxes = box['boxes']\n",
    "        show_bounding_boxes(img, box['boxes'])\n",
    "        show_bounding_boxes(img, label['boxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
